\documentclass[a4paper,11pt,DIV=16]{scrartcl}
\usepackage{ucs}
\usepackage{array}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[normalem]{ulem}
\usepackage{booktabs}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}\hypersetup{colorlinks=true,hypertexnames=false}
\usepackage[osf,sc]{mathpazo}
\usepackage{booktabs}
\usepackage{graphicx}

\usepackage[usenames,dvipsnames]{xcolor}\definecolor{bg}{rgb}{0.95,0.95,0.95}
\usepackage[english]{babel}
\usepackage{minted}\usemintedstyle{emacs}
\usepackage{listings}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{pgfgantt}
\usepackage{afterpage}

\usepackage{./tikzuml-v1.0-2016-03-29/tikz-uml}
\usetikzlibrary{positioning}
\tikzumlset{font=\scriptsize}

\title{Report : Operational Feature Selection in Gaussian Mixture Models}


\author{Adrien Lagrange
\\
\small{Dynafor}}

\date{\today}

\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \newpage}

\newcommand*{\equl}[2]{%
    \textcolor{#1}{\underline{#2}}%
}

\newtheorem{prop}{Proposition}

\begin{document}
% \maketitle

\begin{titlepage}
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.15\textwidth]{Fig/ensta.pdf}\par\vspace{1cm} &
        \includegraphics[width=0.15\textwidth]{Fig/upsud.jpg}\par\vspace{1cm} \\
    \end{tabular}

    {\scshape\LARGE ENSTA ParisTech - Université Paris Sud \par}
    \vspace{1cm}
    {\scshape\Large Final year internship\par}
    \vspace{1.5cm}
    {\huge\bfseries Report : Operational Feature Selection in Gaussian Mixture Models\par}
    \vspace{2cm}
    {\Large\itshape Adrien Lagrange\par}
    \vfill
    supervised by\par
    Dr.~Mathieu \textsc{Fauvel}

    \vfill

% Bottom of the page
    {\large \today\par}

    % \afterpage{\blankpage}
\end{titlepage}

\thispagestyle{empty}
\mbox{~}
\vfill
{\color{red}\Large\itshape This document is non-confidential. Thus, it can be broadcast outside in paper or electronic format.}
\newpage

\begin{abstract}
Lorem ipsum\ldots
\end{abstract}
{\bf Keywords:} enter, keyword, here.
\newpage

\tableofcontents
\newpage

\listoffigures
\newpage

\section{Introduction}

With the increasing number of remote sensing missions, the quantity of data available for a given landscape becomes larger and larger. In particular, the European satellite Sentinel-2 was launched successfully recently and the hyperspectral and hypertemporal data produced by this mission will be fully available at the end of 2016\footnote{\url{https://sentinel2.cnes.fr/en/sentinel-2-0}}. Therefore, data is more and more difficult to process because computational and also statistical limits. Hence, in many remote sensing applications, the extraction of features from a large amount of available data is required.

For instance, in land-cover classification, given a set of spatial, temporal and spectral features, it is possible to extract those which are the most discriminant for the purpose of classification as explained in \cite{Guyon:2006:FEF:1208773}. In hyperspectral data analysis from the hundreds of available spectral channels, it is possible to reduce the number of channels to make the processing more efficient.

There are two ways to reduce dimension: features extraction and feature selection. Feature extraction means creating new features in combining the existing ones, for example linear combination as in Principal Component Analysis \cite{jimenez1998supervised}. To the contrary, features selection select a subset of the existing features which has the advantage to be much more interpretable for the end-user. The selected subset of features corresponds to the most important features for the given task.

There is a large diversity of method for feature selection. However, they usually do not scale well with the number of pixels to be processed as shown in \cite{fauvel2015fast}. Nevertheless, methods based on Gaussian Mixture Models (GMM) have several interesting properties described in \cite{webb2003statistical} that make them suitable for feature selection in the context of large amount of data. By taking advantage of their intrinsic properties, it is possible to increase the computational efficiency with respect to standard implementation.

The objectives of the internship is to develop and implement a feature selection method based on GMM. The remaining of this mid-term report is organized as follows.

\section{Organization}
\label{sec:orga}

    \subsection{Objectives}

    Several outcomes are expected of this internship. First, based on the previous work \cite{fauvel2015fast} of M.Fauvel, we want to {\bfseries reproduce the SFS selection method and develop an upgrade version} corresponding to the floating forward selection described in the previous section. This first part will results in a Python code freely available on Github \footnote{\url{https://github.com/Laadr/FFFS}}.

    Then, the aim is collaborate with CNES in order to {\bfseries produce an external module which could be plugged in the open-source library Orfeo Toolbox (OTB) developed by CNES} (\cite{christophe2008orfeo}). This module implements the same methods as the Python version but has to be written in C++ and be compatible with the OTB. The module obtained is a fork of a template furnished by OTB developers and is also available freely on Github \footnote{\url{https://github.com/Laadr/otbExternalFastFeaturesSelection}}.

    Finally, we want to {\bfseries test the efficiency of this selection method and compare it to a kernel-based method} (\cite{camps2010remote}). The testing is conducted first on hyperspectral images where features are directly the spectral band and secondly on remote sensing images where numerous and various features are used (texture features, HoG, morphological profiles, radiometric indexes, ...).

    \subsection{Planning}

    As can be expected, the cycle is conducted from high-level to low-level which means that the Python code is first developed and so allow us to set definitively the structure of the algorithm. The low-level C++ code is then developed in order to assure good performances and to respect the interface with the OTB.

    Each implementation is followed by a testing period during which the code is tested with artificial and easily manipulable data. Moreover, the development of the C++ code is done in interaction with developers of the OTB and, in particular, a first meeting is organized at the early stage of the development to assure that the best choice are made and a second meeting is set during the validation of the C++ code to get feedbacks.

    The experimentation are conducted at the end to demonstrate the interest of the project. The Gantt diagram \ref{gantt} summarized the expected organization of the project.

    \begin{figure}[ht]
        \centering
        \resizebox{\linewidth}{!}{
            \begin{ganttchart}[hgrid,
                vgrid,
                bar/.append style={fill=blue!30},
                group/.append style={draw=black,fill=red!50},
                title/.append style={fill=gray!20}]{1}{48}
                \gantttitle{\bfseries April}{8}\gantttitle{\bfseries May}{8}\gantttitle{\bfseries June}{8}\gantttitle{\bfseries July}{8}\gantttitle{\bfseries August}{8}\gantttitle{\bfseries September}{8} \\
                \ganttgroup{Development}{1}{26}\\
                \ganttbar{\em Python Implementation}{1}{8} \\
                \ganttbar{\em C++ Implementation}{9}{26} \\
                \ganttmilestone{\em Meeting CNES}{6}\\
                \ganttmilestone{\em Orfeo Toolbox Users Days (CNES)}{11}\\
                \ganttgroup{Testing}{7}{34}\\
                \ganttbar{\em Validation Python code}{7}{18}\\
                \ganttbar{\em Validation C++ code}{25}{34}\\
                \ganttmilestone{\em Meeting CNES}{26}\\
                \ganttgroup{Experimentation on real data}{20}{48}\\
                \ganttbar{\em Experimentation on hyperspectral images}{20}{48}\\
                \ganttbar{\em Experimentation with heterogeneous features}{32}{48}\\
                \ganttmilestone{\em Mid-term report}{24}\\
                \ganttmilestone{\em Final report}{47}
            \end{ganttchart}}
        \caption{Gantt diagram of the internship.}
        \label{gantt}
    \end{figure}

\section{Features selection with Gaussian Mixture Models}
\label{sec:theory}

In the remaining, the following notations are used. $\mathcal{S} = \{\mathbf{x}_i,y_i\}_{i=1}^{n}$ is the training set where $\mathbf{x}_i \in \mathbb{R}^d$ is the vector of features of the $i^{th}$ sample, $y_i = 1,...,C$ the associated label, C the number of labels, $n$ the number of samples and $n_c$ the number of samples of class $c$.

    \subsection{Gaussian Mixture Models}

    The hypothesis of Gaussian mixture models (GMM) is that a given sample is the realization of a random vector which distribution is a mixture (convex combination) of several class conditioned distribution:
    \begin{equation}
        p(\mathbf{x}) = \sum_{c=1}^{C} \pi_c f_c(\mathbf{x}|\theta)
    \end{equation}
    where $\pi_c$ is the prior i.e. the proportion of class $c$ and $f_c$ a probability density function parametrized by $\theta$.

    The Gaussian model assumes that each $f_c$ is , conditionally to $c$, the probability density of a Gaussian distribution of parameters $\boldsymbol{\mu}_c$ and $\boldsymbol{\Sigma}_c$ and so $f_c(\mathbf{x}|\theta)$ can be written
    \begin{equation*}
        f_c(\mathbf{x}|\theta) = \frac{1}{(2\pi)^{\frac{d}{2}} |\boldsymbol{\Sigma}_c|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right).
    \end{equation*}

    Such a model is used in the case of supervised learning and the class parameters $\boldsymbol{\mu}_c$ and $\boldsymbol{\Sigma}_c$ can be estimated using the training samples. In our work, we choose to compute them with the conventional unbiased empirical estimator
    \begin{align}
        \hat{\pi}_c &= \frac{n_c}{n},\\
        \hat{\boldsymbol{\mu}}_c &= \frac{1}{n_c} \sum_{\{i|y_i = c\}} \mathbf{x}_i ,\\
        \hat{\boldsymbol{\Sigma}}_c &= \frac{1}{(n_c - 1)} \sum_{\{i|y_i = c\}} (\mathbf{x}_i - \boldsymbol{\mu}_c) (\boldsymbol{}x_i - \boldsymbol{\mu}_c)^t.
    \end{align}

    To determine the class of a samples, the maximum a posteriori (MAP) rule is used and thus, if we simplify with Bayes' law, the decision rule can be written as
    \begin{align*}
        \mathbf{x} \text{ belongs to } c &\Leftrightarrow c = \text{arg} \max_{c \in C} p(c|\mathbf{x}),\\
                                         &\Leftrightarrow c = \text{arg} \max_{c \in C} \frac{p(c) p(\mathbf{x}|c)}{p(\mathbf{x})},\\
                                         &\Leftrightarrow c = \text{arg} \max_{c \in C} p(c) p(\mathbf{x}|c).\\
    \end{align*}
    By taking the log, a simplified decision formula is obtained
    \begin{align}
        Q_c(\mathbf{x})
        &= 2 \log \left( p(c) p(\mathbf{x}|c) \right) \nonumber \\
        &= 2 \log \left( \pi_c \frac{1}{(2\pi)^{\frac{d}{2}} |\boldsymbol{\Sigma}_c|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right) \right) \nonumber \\
        &= 2 \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right) + 2 \log \left( \frac{1}{(2\pi)^{\frac{d}{2}} |\boldsymbol{\Sigma}_c|^{\frac{1}{2}}} \right) + 2 \log (\pi_c) \nonumber \\
        &= - (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) - \log (|\boldsymbol{\Sigma}_c|) + 2 \log (\pi_c) - d \log (2\pi).
        \label{eq:decision}
    \end{align}

    It is interesting to notice that the covariance, its inverse and its determinant are a key element of the decision function and that the estimation of these elements suffer from the curse of dimensionality. More precisely, the number of parameters to estimate (mean vectors, covariance matrices, proportions) increases quadratically relatively to the number of features and we need at least as many samples as parameters to make an estimation which can be an issue. For example, with hyperspectral data, we face high dimensional samples but very few samples are labeled available because of the difficulty and the cost to collect ground-truth.

    A lack of samples induces ill-conditioned covariance matrices and so unstable inversion and computation of the determinant. There are two major solutions to this problem. First option is to use a regularization method to stabilize the inversion of the covariance matrices. Second option is to use a features extraction method in order to reduce the dimensionality of the samples.

    In our study based on GMM, a Ridge regularization method described in Section \ref{sec:regularization} is implemented. Then we focus on a feature selection method named sequential forward features selection presented in Section \ref{sec:forward-presentation} and an improvement of this algorithm, the sequential floating forward features selection introduced in Section \ref{sec:floating-presentation}.

    In order to evaluate the benefits of such methods for classification, we introduce in the next section several functions called criterion functions.

    \subsection{Criterion function}
    \label{sec:criterion}

    Criterion functions aim to evaluate either a rate of good classification or the separability/similarity of class distributions. These functions are used to estimate which sets of variables are the best to represent data, to assure the separability of the classes and to perform classification. The choice of a criterion function is the choice of a way to compare sets of variables.

        \subsubsection{Measures of good classification}
        \label{sec:criterion-rate}

        As described in \cite{congalton2008assessing} (chapter 4), all this measures of good classification are based on an error matrix $M$ called confusion matrix which is defined so that $M_{ij}$ is the number of samples of class $i$ classified as class $j$. The confusion matrix allows the computation of several interesting values relatively to each class the number of True Positive (TP) corresponding to good prediction, True Negative (TN) corresponding to good classification of the other class, False Negative (FN) corresponding to the samples of the class labeled as an other class and the False Positive (FP) corresponding to the samples wrongly classified as part of of this class. Figure~\ref{fig:confusion} illustrates the definition of this values.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
              \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ value}} &
                & \multicolumn{3}{c}{\bfseries Prediction outcome} \\
              & & \bfseries $c_1$ & \bfseries $c_2$ & \bfseries $c_3$ \\
              & $c_1'$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} & \MyBox{False}{Negative} \\[2.4em]
              & $c_2'$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} & \MyBox{~}{~} \\[2.4em]
              & $c_3'$ & \MyBox{False}{Positive} & \MyBox{~}{~} & \MyBox{True}{Negative}
            \end{tabular}
            \caption{Confusion matrix with TP, TN, FP and FN relatively to $c_1$}
            \label{fig:confusion}
        \end{figure}

        \paragraph{The overall accuracy} is simply the rate of the number of samples with the correct predicted label over the number of samples. This metric is easy to interpret but is biased in the case of unbalanced classes.

        \paragraph{The Cohen's kappa} is a statistic which measure the probabilities of agreement between predictions and ground-truth. This metric tends to give an equal importance to each class.

        \paragraph{The mean F1 score} is the average of the F1 score for each class and the F1 score is the harmonic mean of the precision (number of True Positive over True Positive plus False Positive) and the recall (number of True Positive over True Positive plus False Negative).

        \begin{align}
            \text{Overall Accuracy} &= \frac{\text{TP}}{n} \\
            \text{Kappa} &= \frac{pa - pr}{1 - pr}  \\
            \text{F1 Mean} &= \frac{2 \text{TP}}{2 \text{TP} + \text{FN} + \text{FP}}
        \end{align}
        where TP stands for True Positive, FN for False Negative, FP for False Positive, $pa$ is the probability of agreement defined by $pa = Overall Accuracy$  and $pr$ the probability of random agreement defined by $pr = \sum_{c=1}^{C} \frac{TP_c}{FP_c} \frac{TP_c}{FN_c}$.

        In order to estimate classification rate, the GMM has to be trained and tested on separate datasets. In our case, we choose to use a cross-validation process over our training set. Thus, the training set is divided in k folds, then, the training is done with $(k-1)$ folds and the performances are estimated with the remaining fold. For more details about cross-validation, see \cite{opac-b1127878} (Chapter 7.10).

        \subsubsection{Measures of similarity between distributions}

        The second type of criterion functions is a measure of distance between two distributions. These measures are called divergence function and are defined so that, if we note $S$ a space of all probability distribution with same support, it verifies
        \begin{align*}
            &\forall (p,q) \in S, \text{Div}(p,q) \geq 0, \\
            &\text{Div}(p,q) = 0 \Leftrightarrow p = q.
        \end{align*}
        More specifically, we focus on two particular divergence: the Kullback–Leibler divergence and the Jeffries–Matusita distance. The advantage of these divergence is that they have a simplified expression in the case of Gaussian model. The simplification allows us to get rid of any integration calculus which could be a major problem when dealing with high-dimensional data.


        \paragraph{The Kullback–Leibler divergence} measures the amount of information lost when the first distribution is approximated by the second one. The formal definition is
        \begin{equation}
            \text{Div}_{KL}(c_i,c_j) = \int_\mathbf{x} p(\mathbf{x}|c_i) \ln(\frac{p(\mathbf{x}|c_i)}{p(\mathbf{x}|c_j)}) d\mathbf{x}.
        \end{equation}
        And in the case of Gaussian model, it can be rewritten as follows
        \begin{align}
            \text{Div}_{KL}(c_i,c_j) = \frac{1}{2} \left( \text{Tr} (\boldsymbol{\Sigma}_{c_i}^{-1} \boldsymbol{\Sigma}_{c_j}) + (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_j})^t \boldsymbol{\Sigma}_{c_i}^{-1} (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_j}) - d + \log \left( \frac{|\boldsymbol{\Sigma}_{c_i}|}{|\boldsymbol{\Sigma}_{c_j}|} \right) \right)
        \end{align}
        where Tr is the trace operator and $d$ the dimension of the distribution.

        It can be noticed that the KL divergence is not symmetric, i.e. $\text{Div}_{KL}(c_i,c_j) \ne \text{Div}_{KL}(c_j,c_i)$, and so the symmetrized version is used to compute the criterion function. In the case of Gaussian model, the symmetrization induces the following simplification of the formula
        \begin{align}
            \text{SKL}_{ij} &=\text{Div}_{KL}(c_i,c_j) + \text{Div}_{KL}(c_j,c_i) \nonumber \\
            &= \frac{1}{2} \left( \text{Tr} (\boldsymbol{\Sigma}_{c_i}^{-1} \boldsymbol{\Sigma}_{c_j} + \boldsymbol{\Sigma}_{c_j}^{-1} \boldsymbol{\Sigma}_{c_i}) + (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_j})^t (\boldsymbol{\Sigma}_{c_i}^{-1} + \boldsymbol{\Sigma}_{c_j}^{-1}) (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_j}) - 2d \right)
        \end{align}

        Moreover, the divergence is computed between two classes and to obtain a unique value the weighted mean of divergence measures is taken
        \begin{equation}
            \text{C}_{SKL} = \sum_{i=1}^{C} \sum_{j=i + 1}^{C} \pi_{c_i} \pi_{c_j} \text{SKL}_{ij}
        \end{equation}

        \paragraph{The Bhattacharyya distance} is defined as follows
        \begin{equation}
            \text{B}_{ij} = - \ln \left( \int_\mathbf{x} \sqrt{p(\mathbf{x}|c_i) p(\mathbf{x}|c_j)} d\mathbf{x} \right)
        \end{equation}
        And in the case of Gaussian model:
        \begin{equation}
            \text{B}_{ij} = \frac{1}{8} (\boldsymbol{\mu}_i - \boldsymbol{\mu}_j)^t \left( \frac{\boldsymbol{\Sigma}_i + \boldsymbol{\Sigma}_j}{2} \right)^{-1} (\boldsymbol{\mu}_i - \boldsymbol{\mu}_j) + \frac{1}{2} \ln \left( \frac{|\boldsymbol{\Sigma}_i + \boldsymbol{\Sigma}_j|}{\sqrt{|\boldsymbol{\Sigma}_i| |\boldsymbol{\Sigma}_j|}} \right)
        \end{equation}

        \paragraph{The Jeffries–Matusita distance} is a measure based on the Bhattacharyya distance but transform in a way that the distance saturate if the separability between the two distribution increases. The JM distance is defined according to
        \begin{equation}
            \text{JM}_{ij} = \sqrt{ \int_\mathbf{x} \left[\sqrt{p(\mathbf{x}|c_i)} - \sqrt{p(\mathbf{x}|c_j)}\right]^2 d\mathbf{x} }
        \end{equation}
        And the Jeffries–Matusita distance can be rewritten according to the Bhattacharyya distance
        \begin{equation}
            \text{JM}_{ij} = \sqrt{ 2 \{1 - \text{exp}[-B_{ij}]\} }
        \end{equation}

        As for the KL divergence, a weighted mean of the distance between two classes is computed to aggregate the measures in a single value.

        \vspace{10 mm}

        According to \cite{bruzzone2009novel}, it is interesting to notice that the KL divergence increases quadratically with respect to the distance between the mean vectors of the class distributions whereas the measures of good classification we used asymptotically tends to one when distributions are perfectly separable. On the contrary, the JM distance tends to saturate as these measures of good classification.


\section{Ridge Regularization}
\label{sec:regularization}

    \subsection{Principe of Ridge Regularization}
    As introduced in the previous section, the aim of a regularization method is to stabilize the inversion of the covariance matrix in the context of high dimensionality which often imply a badly-conditioned matrix.

    The Ridge regularization is a particular case of the Tikhonov regularization introduced in \cite{hoerl1970ridge} which aim to penalize the linear problem used to compute the inverse of the covariance matrix by the norm of the inverse. We want to obtain the regularized inverse by solving
    \begin{equation}
        \hat{\mathbf{A}} = \min_\mathbf{A} \lVert\boldsymbol{\Sigma} \mathbf{A} - \mathbf{I}\rVert^2 + \lVert\boldsymbol{\Gamma}\mathbf{A}\rVert^2.
    \end{equation}
    In deriving the expression and looking at the minimum, we find that the explicit solution to this problem is
    \begin{equation}
        \hat{\mathbf{A}} = (\boldsymbol{\Sigma}^t\boldsymbol{\Sigma} + \boldsymbol{\Gamma}^t \boldsymbol{\Gamma})^{-1} \boldsymbol{\Sigma}.
    \end{equation}
    There are then two cases, either we choose $\boldsymbol{\Gamma} = \sqrt{\tau} \mathbf{I}$ which corresponds to the Tikhonov regularization or $\boldsymbol{\Gamma} = \sqrt{\tau} \boldsymbol{\Sigma}^{\frac{1}{2}}$ which corresponds to the Ridge regularization with $\tau$ a scalar. Then, we get
    \begin{align}
        \text{Tikhonov: }& \hat{\mathbf{A}} = (\boldsymbol{\Sigma}^2 + \tau \mathbf{I})^{-1} \boldsymbol{\Sigma},\\
        \text{Ridge: }& \hat{\mathbf{A}} = (\boldsymbol{\Sigma} + \tau \mathbf{I})^{-1}.
    \end{align}

    It is interesting to see the effect of the regularization method through the eigenvalues of the regularized inverse. Thus, we obtain the following expression
    \begin{align}
        \text{Tikhonov: }& \hat{\lambda}_i^{-1} = \frac{\lambda_i}{\lambda_i^2 + \tau},\\
        \text{Ridge: }& \hat{\lambda}_i^{-1} = \frac{1}{\lambda_i + \tau},
    \end{align}
    where $\lambda_i$ is the ith eigenvalue of the covariance matrix and $\hat{\lambda}_i^{-1}$ the eigenvalue of the regularized inverse.

    With the help of Figure~\ref{fig:eigenvalues}, we can see that the difference between these two regularization is the way to treat small eigenvalues. The Ridge regression has two advantages. First, it is monotonous and secondly it does not converge to zero when eigenvalue tends to zero.

    \begin{figure}[!ht]
        \centering
        \includegraphics[width=0.7\textwidth]{Fig/eigenval.pdf}
        \caption{Consequence of regularization on eigenvalues.\label{fig:eigenvalues}}
    \end{figure}

    In particular, the Ridge regularization proposes to solve the problem in adding a small value $\tau$ to each eigenvalues of the covariance matrix. In our case, we choose to add the same constant to each eigenvalues but it is not compulsory in a general case.

    Thus, in practice, we choose to use Ridge regularization. To perform the regularization, the covariance matrix is decomposed with a diagonalization algorithm for symmetric matrices s.t. $\boldsymbol{\Sigma} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^t$ where $\mathbf{Q}$ is the matrix of eigenvectors and $\boldsymbol{\Lambda}$ is the diagonal matrix of eigenvalues. When the decomposition is available, the regularization almost finish and the decision function is simply rewritten as follow

    \begin{align}
    \label{eq:decision-ridge}
        Q_c(\mathbf{x}) = - (\mathbf{x} - \boldsymbol{\mu}_c)^t \mathbf{Q}_c (\boldsymbol{\Lambda}_c + \tau \mathbf{I})^{-1} \mathbf{Q}_c^t (\mathbf{x} - \boldsymbol{\mu}_c) - \log (|\boldsymbol{\Sigma}_c + \tau \mathbf{I}|) + 2 \log (\pi_c) - d \log (2\pi),
    \end{align}
    where $\mathbf{I}$ is the identity matrix. It is to be noticed that $(\boldsymbol{\Lambda} + \tau \mathbf{I})$ is also a diagonal matrix and so the inverse is easily obtained in inverting each diagonal element of the matrix.

    \subsection{Implementation}

    The main steps of the training algorithm when using Ridge regularization are summarized in Algorithm \ref{alg:ridge}. Indeed, before training the GMM model, a preliminary step is needed in order to determine a good value for the parameter $\tau$ and to do so, a gridsearch is performed with a set of values $\mathcal{P}$ given by the user. To estimate the quality of the classification with a given $\tau$, one of the good classification criterion presented in Section~\ref{sec:criterion-rate} is used. As explained at the end of the same section, the classification rate is computed with a cross-validation process.

    A particularity of this regularization method is used to accelerate the gridsearch. More precisely, the two computationally costly step are the computation of the inverse and the determinant of the regularized covariance matrices and it is important to see that these operations can be performed only once to test all $\tau$. The covariance matrix is diagonalized before selecting $\tau$ and the eigenvalues and eigenvectors are stored and the regularized inverse and determinant can be update as follows
    \begin{align}
        \hat{\boldsymbol{\Sigma}}_c^{-1} &= \mathbf{Q}_c (\boldsymbol{\Lambda}_c + \tau \mathbf{I})^{-1} \mathbf{Q}_c^t, \nonumber\\
        \log (|\hat{\boldsymbol{\Sigma}}_c|) &= \log (|\boldsymbol{\Sigma}_c + \tau \mathbf{I}|) = \sum_{i=1}^{d} \log (\lambda_i + \tau).
        \label{eq:update-reg}
    \end{align}

    \begin{algorithm}
    \caption{Ridge Regularization training steps\label{alg:ridge}}
    \begin{algorithmic}[1]
    \REQUIRE $\mathcal{S},k,\mathcal{P}$
    \STATE Learn GMM with $\mathcal{S}$
    \STATE Randomly cut $\mathcal{S}$ in $k$ subsets such as $\mathcal{S}_1 \cup \text{...} \cup \mathcal{S}_k = \mathcal{S}$ and $\mathcal{S}_i \cap \mathcal{S}_j = \emptyset$
    \FORALL{$\mathcal{S}_i$}
    \STATE Update GMM model for $i^{th}$ fold with Equations \ref{eq:update-cv1} and \ref{eq:update-cv2}
    \STATE Compute decomposition of covariance matrices of each class s.t. $\boldsymbol{\Sigma}_c = \mathbf{Q}_c \boldsymbol{\Lambda}_c \mathbf{Q}_c^t$
    \FORALL{$\tau \in \mathcal{P}$}
    \STATE Update inverse and logdet with Equations \ref{eq:update-reg}
    \STATE Estimate classification rate on $\mathcal{S}_i$ using Equation \ref{eq:decision-ridge}
    \ENDFOR
    \ENDFOR
    \STATE Average the classification rate over the $k$ folds
    \STATE Compute $\text{arg} \max_{\tau}$ of the classification rates to get the best parameter $\tau^*$
    \STATE Parametrize the GMM with $\tau^*$
    \end{algorithmic}
    \end{algorithm}

    Finally, another trick could used to optimize the gridsearch. As explained in Section~\ref{sec:update-cv}, it is possible to avoid the learning step on the $(k-1)$ fold and instead learn the model with the whole dataset and deduce the submodel used for cross-validation from the complete model and the mean vector and covariance matrix of the $k^{th}$ fold.

    As stated before, we choose to implement a basic Ridge regression method and only the small improvement explained herebefore as been proposed. To the contrary, we choose in our study to focus more on a feature selection method named sequential forward features selection described in following section.

\section{Features selection}

The objective of feature selection method is to select a subset of variables to describe each sample and so to get rid of high dimensionality and its curse.

Features selection algorithm may be divided into three types. The first type which called filter method is based uniquely on data analysis. Features are ranked according to a statistical analysis of the data. For example, the Principal Component Analysis (PCA) described in \cite{jimenez1998supervised} is a typical filter method or else various metrics \cite{bruzzone1995extension}, \cite{biesiada2007feature}, \cite{demir2008phase}.

The second sort are known as wrapper methods which can be seen as search method to determine the best set of variables for a given learning model. As exhaustive searches are out of question in a practical amount of time, numerous search strategies have been designed some optimal under particular hypothesis \cite{narendra1977branch} and other suboptimal but easier to set up \cite{whitney1971direct}, \cite{somol1999adaptive}. The advantage of such method compare to filter method is that they are dedicated to a particular model but on the other hand, as they require the training of multiple models to test various set of variables, they tend to be slower.

The third type corresponds to the embedded method which do not separate the features selection process from the learning algorithm and allow interaction between the two processes. The basic example of such method is the decision tree algorithm in which a feature is selected for creation of each node. Embedded methods also exist for other model, e.g. SVM (\cite{guyon2002gene}, \cite{weston2003use}).

The selection method proposed in this work in a wrapper method associated to GMM models. It is important to underline that this method is a \emph{selection} method and not an \emph{extraction} method. It means the subset of variables obtained is composed of actual variables of the original set and not variables built as combination of several others as it is the case for example with PCA. This choice is made to assure an easier interpretation by the user of the obtained subset of variables.

Two selection algorithms are presented the sequential forward features selection method (Section \ref{sec:forward-presentation}) and the sequential floating forward feature selection method (\ref{sec:floating-presentation}) the second being a more complex variation of the first.

    \subsection{Sequential forward features selection}
    \label{sec:forward-presentation}

    The Sequential Forward Selection (SFS) starts with an empty set of selected features. Then it tests at each step for all the remaining features the value of a criterion function $J$ chosen among the ones presented in Section \ref{sec:criterion} when the feature is added to the pool of selected features. The feature that maximizes the criterion function is definitively added to the pool of selected features and it moves to the next iteration. The algorithm stops when a given number of variables \emph{maxVarNb} has been selected.

    To summarize, the features are selected one by one and the set of selected features are never questioned. It results in a reasonable computational time but a suboptimal solution to the selection problem meaning that a better subset of variables could exist.

    \begin{algorithm}
    \caption{Sequential forward features selection\label{alg:sfs}}
    {\fontsize{10}{10}\selectfont
    \begin{algorithmic}[1]
    \REQUIRE $\mathcal{S},J,\text{maxVarNb}$
    \STATE $S=\emptyset$
    \STATE $F=\text{\{all variables\}}$
    \WHILE{$\text{card}(S) \leq maxVarNb$}
    \FORALL{$f_i \in F$}
    \STATE $R_i = J(\{S + f_i\})$
    \ENDFOR
    \STATE $j=\text{arg} \max_{i} R_i$
    \STATE $S = \{S + f_j\}$
    \STATE $F = F \setminus f_j$
    \ENDWHILE
    \RETURN $S$
    \end{algorithmic}
    }
    \end{algorithm}


    Figure~\ref{fig:expl_dataset} shows a toy dataset with 2 classes and 2 features. The projection of the Gaussian distribution learned for each class can be visualized on the corresponding axis. In this examples, each class has been generated given a Gaussian law but it is not necessary in real case.

    \begin{figure}[!ht]
        \centering
        \begin{tabular}{cc}
            \includegraphics[width=0.5\textwidth]{Fig/example_case1.pdf} &
            \includegraphics[width=0.5\textwidth]{Fig/example_case2.pdf} \\
            {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \multicolumn{2}{c}{\includegraphics[width=0.5\textwidth]{Fig/example_case3.pdf}} \\
            \multicolumn{2}{c}{{\bfseries{(c)}}} \\
        \end{tabular}
        \caption{Example datasets: {\bfseries{(a)}} 2 classes and 2 features but only one informative feature, {\bfseries{(b)}} same dataset with a reduced gap between means of informative feature, {\bfseries{(c)}} similar dataset with unbalanced classes (350/50 samples).\label{fig:expl_dataset}}
    \end{figure}

    In the cases of the example presented in Figure~\ref{fig:expl_dataset}, we compute the criterion functions described in \ref{sec:criterion} and summarize it in Table \ref{tab:expl_metric}. We can see that all the criterion functions are maximum with the feature 1 which makes sense because the projection of the learned Gaussian distribution effectively seems separable only for feature 1. So the algorithm will has expected select the feature 1 as the best features to perform classification. The case (c) also shows that the measures are robust to unbalanced classes except the overall accuracy which still correctly ranks features in this case but makes less difference between features 1 and 2 because of the imbalance.

    \begin{table}[!ht]
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{l|r|r|r|r|r|r|r|r|r}
             & \multicolumn{3}{c|}{Case a} & \multicolumn{3}{c|}{Case b} & \multicolumn{3}{c}{Case c} \\
            \hline
             & Feat 1 & Feat 2 & Feat 1+2 & Feat 1 & Feat 2 & Feat 1+2 & Feat 1 & Feat 2 & Feat 1+2 \\
            \hline
            Overall Accuracy          & 0.995 & 0.445 & 0.995 & 0.935 & 0.445 & 0.925 & 0.985 & 0.87 & 0.985 \\
            Cohen's kappa             & 0.99  & -0.11 & 0.99  & 0.87  & -0.11 & 0.85  & 0.95  & 0    & 0.95 \\
            F1-score mean             & 0.995 & 0.43  & 0.995 & 0.935 & 0.43  & 0.92  & 0.97  & 0.47 & 0.97 \\
            KL divergence             & 13.04 & 0.25  & 13.44 & 3.48  & 0.25  & 3.77  & 2.66  & 0.12 & 2.79 \\
            Jeffrey-Matusita distance & 0.35  & 0.19  & 0.35  & 0.31  & 0.19  & 0.32  & 0.16  & 0.09 & 0.16 \\
        \end{tabular}}
        \caption{Criterion functions values corresponding to Figure~\ref{fig:expl_dataset} dataset (50\% for training/50\% for testing).\label{tab:expl_metric}}
    \end{table}


    \subsection{Sequential floating forward feature selection}
    \label{sec:floating-presentation}

    The Sequential Floating Forward Selection (SFFS) is actually based on two algorithms: the SFS described above and the Sequential Backward Selection (SBS). The SBS is the backward equivalent of SFS. The difference is that it starts with every features in the pool of selected features and tries at each step to remove the less significant one in term of the given criterion function.

    The SFFS works as the SFS but between each step of the SFS algorithm a backward selection is operated. At the end of the SBS step, the value of the criterion function is compared to the best value ever obtained with a set of features of the same size and if the new value is better the feature puts into question is effectively taken away and the next step is again a SBS but if the new value is not better the SBS step is forgotten and it moves to the next SFS step. The algorithm stops when a given number of features has been selected.

    This SFFS algorithm eventually tests more solutions than the SFS algorithm. The results are expected to be better but the trade-off is an increased computational time which is dependent on the complexity of the dataset.

    \begin{algorithm}
    \caption{Sequential floating forward features selection\label{alg:sffs}}
    {\fontsize{10}{10}\selectfont
    \begin{algorithmic}[1]
    \REQUIRE $\mathcal{S},J,\text{maxVar}$
    \STATE $S=\emptyset$
    \STATE $F=\text{\{all variables\}}$
    \STATE $k=0$
    \WHILE{Repeat till $S$ contained $\text{maxVar}$ variables}
    \FORALL{$f_i \in F$}
    \STATE $R_i = J(\{S{(k)} + f_i\})$
    \ENDFOR
    \STATE $j=\text{arg} \max_{i} R_i$
    \IF{$R_j \geq J(S^{(k+1)})$}
    \STATE $k=k+1$
    \ELSE
    \STATE $S^{(k+1)} = \{S{(k)} + f_i\}$
    \STATE $k=k+1$
    \STATE $\text{flag}=1$
    \WHILE{$k > 2 \text{ and } \text{flag}=1$}
    \FORALL{$f_i \in S^{(k)}$}
    \STATE $R_i = J(\{S{(k)}\setminus f_i\})$
    \ENDFOR
    \STATE $j=\text{arg} \max_{i} R_i$
    \IF{$R_j < J(S^{(k-1)})$}
    \STATE $S^{(k-1)} = \{S{(k)} \setminus f_i\}$
    \STATE $k=k-1$
    \ELSE
    \STATE $\text{flag}=0$
    \ENDIF
    \ENDWHILE
    \ENDIF
    \ENDWHILE
    \RETURN $S$
    \end{algorithmic}
    }
    \end{algorithm}

    \subsection{Implementation}
    A major contribution of our work is the optimization in term of computational efficiency. More precisely, three steps of our method has been upgraded in order to reduce computational time. First of all, the GMM model is learned only once using samples. If a covariance matrix or a mean vector of a reduced set of variables is required, we get it from the global model learned at the beginning.

    Secondly, when a cross-validation is performed, we do not learn a submodel, i.e. a GMM model trained with $(n-1)$ folds, using the $(n-1)$ folds but, instead, we derive it from the global model and the covariance matrices and mean vectors of the fold used for validation for this given submodel. The process is described in details in Section~\ref{sec:update-cv}.

    Finally and most importantly, when variables are tested one by one during a selection step, costly operations are made when computing criterion functions for each variable especially the computation of the inverse of covariance matrices and its determinant. We manage to set up several update rules which allow us to compute this inverse and determinant only once to test all variables. These update rules are presented in Section~\ref{sec:update-crit}.

        \subsubsection{Update for cross validation}
        \label{sec:update-cv}

        Based on \cite{fauvel2015fast}, a method to accelerate the k-fold cross-validation process in the case of criterion functions based on correct classification measures was implemented. The idea is to estimate the GMM model with the whole training set and then, instead of training a model on $(k-1)$ folds, the complete model and the mean vector and the covariance matrix of the $k^{th}$ fold is used to derive the corresponding submodel.

        The following formulae can be obtained (details of calculation in Appendix \ref{app:cv_maj})
        \begin{prop}
            \label{eq:update-cv1}
            (CV mean update)
            \begin{equation*}
                \boldsymbol{\mu}_c^{n_c-\nu_c} = \frac{n_c \boldsymbol{\mu}_c^{n_c} - \nu_c \boldsymbol{\mu}_c^{\nu_c}}{n_c - \nu_c} \nonumber
            \end{equation*}
        \end{prop}
        \begin{prop}
            \label{eq:update-cv2}
            (CV covariance matrix update)
            \begin{equation*}
                \boldsymbol{\Sigma}_c^{n_c-\nu_c} = \frac{1}{n_c-\nu_c-1} ( (n_c-1) \boldsymbol{\Sigma}_c^{n_c} - (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} - \frac{n_c \nu_c}{(n_c-\nu_c)} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t) \nonumber
            \end{equation*}
        \end{prop}
        where $\nu_c$ is the number of samples of class $c$ removed from the set.

        Additionally, the update of the GMM model is the occasion to precompute and store the constant $2 \log (\pi_c)$ which is a term of the decision function constant for a given training set.

        \subsubsection{Criterion function computation}
        \label{sec:update-crit}

        As explained earlier, at each iteration the SFS and SFFS algorithms compute the value of a criterion function for every possible set of features composed of the selected ones augmented by one of the remaining features. One of the main achievements of this work is to reduce the computational time needed to compute the criterion function of augmented sets.

        We note $\boldsymbol{\Sigma}^{(k-1)}$ a covariance matrix of the $k-1^{th}$ iteration, i.e., a covariance of the selected features and $\boldsymbol{\Sigma}^{(k)}$ the corresponding covariance matrix at the $k^{th}$ iteration, i.e., the covariance of the augmented set. Then, the inverse of the covariance matrix $(\boldsymbol{\Sigma}^{(k)})^{-1}$, the quadratical term $(\mathbf{x}^{(k)})^t (\boldsymbol{\Sigma}^{(k)})^{-1} \mathbf{x}^{(k)}$ and the determinant $\log |\boldsymbol{\Sigma}^{(k)}|$ can be expressed in function of terms of the $(k-1)^{th}$ iteration. These calculi use the fact that the covariance matrix is a positive definite symmetric matrix to simplify formulae using matrices defined by blocks. These update rules are summed up hereafter and are available in \cite{webb2003statistical} (chapter 9.2).

        In order to easily understand which are available and which ones needs to be computed, we choose to underline and set in red the terms already computed in the update rules.

        As $\boldsymbol{\Sigma}^{(k)}$ is a positive definite symmetric matrix, we can use the following notation
        \begin{equation*}
            \boldsymbol{\Sigma}^{(k)} =
            \bigg[\begin{array}{cc}
            \boldsymbol{\Sigma}^{(k-1)} & \mathbf{u}      \\
            \mathbf{u}^t          & \sigma_{kk} \\
            \end{array}\bigg]
        \end{equation*}
        where $\mathbf{u}$ is the $k^{th}$ column of the matrix without the diagonal element i.e. $\mathbf{u}_{i} = \boldsymbol{\Sigma}^{(k)}_{i+1,k}$ with $i \in [1,k-1]$.

        Using the formula of the inverse of a block matrix, the following formula expressing $(\boldsymbol{\Sigma}^{(k)})^{-1}$ in function of $(\boldsymbol{\Sigma}^{(k-1)})^{-1}$ is obtained
        \begin{prop}
        \label{eq:update-inv}
            (Forward update rule for inverse of covariance matrix)
            \begin{equation*}
                \fbox{$
                (\boldsymbol{\Sigma}^{(k)})^{-1} =
                \bigg[\begin{array}{cc}
                \equl{red}{(\boldsymbol{\Sigma}^{(k-1)})^{-1}} + \frac{1}{\alpha} \equl{red}{(\boldsymbol{\Sigma}^{(k-1)})^{-1}} \mathbf{u} \mathbf{u}^t \equl{red}{(\boldsymbol{\Sigma}^{(k-1)})^{-1}} & - \frac{1}{\alpha} \equl{red}{(\boldsymbol{\Sigma}^{(k-1)})^{-1}} \mathbf{u} \\
                - \frac{1}{\alpha} \mathbf{u}^t \equl{red}{(\boldsymbol{\Sigma}^{(k-1)})^{-1}}                                                                          & \frac{1}{\alpha}                  \\
                \end{array}\bigg]
                $}
            \end{equation*}
        \end{prop}
        where $ \alpha = \sigma_{kk} - \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} $.

        If we note
        \begin{equation*}
            (\boldsymbol{\Sigma}^{(k)})^{-1} =
            \bigg[\begin{array}{cc}
            \mathbf{A}   & \mathbf{v} \\
            \mathbf{v}^t & \frac{1}{\alpha} \\
            \end{array}\bigg]
        \end{equation*}
        with $\mathbf{A} = (\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1}$ and $\mathbf{v} = - \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u}$.

        In the case of a backward step in SFFS algorithm, we need to invert the formula in order to compute $(\boldsymbol{\Sigma}^{(k-1)})^{-1}$ knowing $(\boldsymbol{\Sigma}^{(k)})^{-1}$. Thus, the update rule becomes
        \begin{prop}
            (Backward update rule for inverse of covariance matrix)
            \begin{equation*}
                \fbox{$(\boldsymbol{\Sigma}^{(k-1)})^{-1} = \equl{red}{\mathbf{A}} - \alpha \mathbf{v} \mathbf{v}^t$}
            \end{equation*}
        \end{prop}

        \begin{proof}
            \begin{align*}
                \mathbf{A} - \alpha \mathbf{v} \mathbf{v}^t
                &= (\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} - \alpha (- \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u}) (- \frac{1}{\alpha} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1})) \\
                &= (\boldsymbol{\Sigma}^{(k-1)})^{-1}
            \end{align*}
        \end{proof}

        A formula can also be deduced for the quadratical term of the decision function. If we note $(\mathbf{x}^{(k)})^t = \left[\begin{array}{cc} (\mathbf{x}^{(k-1)})^t   & x^k \end{array}\right]$, we get the following proposition
        \begin{prop}
        \label{eq:update-quad}
            (Update rule for quadratical term)
            \begin{equation*}
                \fbox{$(\mathbf{x}^{(k)})^t (\boldsymbol{\Sigma}^{(k)})^{-1} \mathbf{x}^{(k)} = \equl{red}{(\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)}} + \alpha ( \left[\begin{array}{cc} \mathbf{v}^t & \frac{1}{\alpha} \end{array}\right] \mathbf{x}^{(k)} )^2$}
            \end{equation*}
        \end{prop}

        \begin{proof}
            \begin{align*}
                (\mathbf{x}^{(k)})^t (\boldsymbol{\Sigma}^{(k)})^{-1} \mathbf{x}^{(k)}
                &= \left[\begin{array}{cc} (\mathbf{x}^{(k-1)})^t   & x_k \end{array}\right]
                \left[\begin{array}{cc}
                \mathbf{A}   & \mathbf{v} \\
                \mathbf{v}^t & \frac{1}{\alpha}
                \end{array}\right]
                \left[\begin{array}{c} \mathbf{v} \\ x_k \end{array}\right] \\
                &= \left[\begin{array}{cc} (\mathbf{x}^{(k-1)})^t \mathbf{A} + x_k \mathbf{v}^t & (\mathbf{x}^{(k-1)})^t \mathbf{v} + \frac{x_k}{\alpha} \end{array}\right]
                \left[\begin{array}{c} \mathbf{v} \\ x_k \end{array}\right] \\
                &= (\mathbf{x}^{(k-1)})^t \mathbf{A} \mathbf{x}^{(k-1)} + x_k \mathbf{v}^t \mathbf{x}^{(k-1)} + (\mathbf{x}^{(k-1)})^t \mathbf{v} x_k + \frac{(x_k)^2}{\alpha} \\
                &= (\mathbf{x}^{(k-1)})^t ((\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1}) \mathbf{x}^{(k-1)}
                   + 2 x_k \mathbf{v}^t \mathbf{x}^{(k-1)} + \frac{(x_k)^2}{\alpha} \\
                &= (\mathbf{x}^{(k-1)})^t ((\boldsymbol{\Sigma}^{(k-1)})^{-1} + \alpha \mathbf{v} \mathbf{v}^t) \mathbf{x}^{(k-1)}
                   + 2 x_k \mathbf{v}^t \mathbf{x}^{(k-1)} + \frac{(x_k)^2}{\alpha} \\
                &= (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( (\mathbf{x}^{(k-1)})^t \mathbf{v} \mathbf{v}^t \mathbf{x}^{(k-1)}
                   + 2 \frac{x_k}{\alpha} \mathbf{v}^t \mathbf{x}^{(k-1)} + \frac{(x_k)^2}{\alpha^2}) \\
                &= (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( (\mathbf{x}^{(k-1)})^t \mathbf{v} + \frac{x_k}{\alpha} )^2 \\
                &= (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( \left[\begin{array}{cc} \mathbf{v}^t & \frac{1}{\alpha} \end{array}\right] \mathbf{x}^{(k)} )^2
            \end{align*}
        \end{proof}

        Finally, using the formula of the determinant of a block matrix and after taking the log, we obtain
        \begin{prop}
        \label{eq:update-log}
            (Update rule for logdet)
            \begin{equation*}
                \fbox{$\log \left(|\boldsymbol{\Sigma}^{(k)}|\right) = \equl{red}{\log \left(|\boldsymbol{\Sigma}^{(k-1)}|\right)} + \log \alpha$}
            \end{equation*}
        \end{prop}

        Now using all update rules, criterion functions can be rewritten  as follows to compute as less times as possible the computationally heavy terms
        \begin{align*}
            Q(\mathbf{x}) = &- \equl{red}{(\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} (\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})} \\
            &- \alpha ( \left[\begin{array}{cc} \mathbf{v}^t & \frac{1}{\alpha} \end{array}\right] (\mathbf{x}^{(k)} - \boldsymbol{\mu}^{(k)}) )^2 \\
            &- \equl{red}{\log \left(|\boldsymbol{\Sigma}^{(k-1)}|\right)} - \log \alpha  + \equl{red}{2 \log (\pi_c)} + \equl{red}{k \log(2 \pi)}
        \end{align*}

        \begin{align*}
            \text{SKL}_{ij} = &\frac{1}{2} \biggl( \text{Tr} ((\boldsymbol{\Sigma}_{c_i}^{(k)})^{-1} \boldsymbol{\Sigma}_{c_j}^{(k)} + (\boldsymbol{\Sigma}_{c_j}^{(k)})^{-1} \boldsymbol{\Sigma}_{c_i}^{(k)})
            + \equl{red}{(\boldsymbol{\mu}_{c_i}^{(k-1)} - \boldsymbol{\mu}_{c_j}^{(k-1)})^t ((\boldsymbol{\Sigma}_{c_i}^{(k-1)})^{-1} + (\boldsymbol{\Sigma}_{c_j}^{(k-1)})^{-1}) (\boldsymbol{\mu}_{c_i}^{(k-1)} - \boldsymbol{\mu}_{c_j}^{(k-1)})} \\
            &+ \alpha ( \left[\begin{array}{cc} \mathbf{v_i}^t & \frac{1}{\alpha_i} \end{array}\right] (\boldsymbol{\mu}_{c_i}^{(k)} - \boldsymbol{\mu}_{c_j}^{(k)}) )^2
            + \alpha ( \left[\begin{array}{cc} \mathbf{v_j}^t & \frac{1}{\alpha_j} \end{array}\right] (\boldsymbol{\mu}_{c_i}^{(k)} - \boldsymbol{\mu}_{c_j}^{(k)}) )^2
            - \equl{red}{2k} \biggr)
        \end{align*}
        with $(\boldsymbol{\Sigma}_{c_i}^{(k)})^{-1}$ computed with Proposition~\ref{eq:update-inv}.

        \begin{align*}
            \text{B}_{ij} = &\equl{red}{\frac{1}{4} (\boldsymbol{\mu}_i^{(k-1)} - \boldsymbol{\mu}_j^{(k-1)})^t ( \boldsymbol{\tilde{\Sigma}}^{(k-1)} )^{-1} (\boldsymbol{\mu}_i^{(k-1)} - \boldsymbol{\mu}_j^{(k-1)})} + \frac{1}{4} \tilde{\alpha} ( \left[\begin{array}{cc} \mathbf{\tilde{v}}^t & \frac{1}{\tilde{\alpha}} \end{array}\right] (\boldsymbol{\mu}_{i}^{(k)} - \boldsymbol{\mu}_{j}^{(k)}) )^2\\
            &+ \equl{red}{\frac{1}{2} \ln \left( \frac{|\boldsymbol{\tilde{\Sigma}}^{(k-1)}|}{\sqrt{|\boldsymbol{\Sigma}_i^{(k-1)}| |\boldsymbol{\Sigma}_j^{(k-1)}|}} \right)} + \frac{1}{2} \ln \left( \frac{\tilde{\alpha}}{\sqrt{\alpha_i \alpha_j}} \right)
        \end{align*}
        where $\boldsymbol{\tilde{\Sigma}} = \boldsymbol{\Sigma}_i + \boldsymbol{\Sigma}_j$

        It can be noticed that these new formulae allow us to precompute the computationally heavy terms because they are the same for all possible augmented sets at a given iteration of the selection. The Algorithm~\ref{alg:sffs-update} illustrates the optimization of the Algorithm~\ref{alg:sffs}.

        \subsection{Computational issues}

        During an iteration of the selection algorithm, it still needs to invert the covariance matrix of the previous iteration. In order to so, we choose to perform a decomposition in eigenvalues and eigenvectors using algorithm specific for symmetric matrices. In other words, we find $\boldsymbol{\Lambda}$ the diagonal matrix of eigenvalues and $\mathbf{Q}$ the matrix of eigenvectors s.t. $\boldsymbol{\Sigma} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^t$.

        This decomposition has several advantages. First, it is very simple to compute the determinant once the eigenvalues are available because the determinant is equal to the product of these values. Secondly, it is possible to check the eigenvalues and so to assure a better computational stability. Due to the curse of dimensionality, the covariance matrix is sometimes badly conditioned and the results is that some eigenvalues are really small (below computational precision) and the decomposition helps us to check their actual values and if they are below EPS\_FLT which the machine maximum precision for a float, we set these eigenvalues to EPS\_FLT. For the same reason, the constant $\alpha$ used in update rules is also thresholded to EPS\_FLT. Algorithm~\ref{alg:sffs-update} details when computational stability is checked.

        \begin{algorithm}
        \caption{Sequential floating forward features selection with updates\label{alg:sffs-update}}
        {\fontsize{8}{8}\selectfont
        \begin{algorithmic}[1]
        \REQUIRE $\mathcal{S},J,\text{maxVar}$
        \STATE $S=\emptyset$
        \STATE $F=\text{\{all variables\}}$
        \STATE $k=0$
        \WHILE{Repeat till $S$ contained $\text{maxVar}$ variables}
        \STATE \underline{Diagonalize $\boldsymbol{\Sigma}^{(k-1)} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^t$}
        \STATE \underline{\bfseries{for all} $\lambda_i$ \bfseries{do} $\lambda_i = \min (\text{EPS\_FLT},\lambda_i)$}
        \STATE \underline{Precompute $(\boldsymbol{\Sigma}^{(k-1)})^{-1}$, $(\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} (\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})$ and $\log \left(|\boldsymbol{\Sigma}^{(k-1)}|\right)$}
        \FORALL{$f_i \in F$}
        \STATE \underline{Compute update constant $\alpha$}
        \STATE \underline{$\alpha = \min (\text{EPS\_FLT},\alpha)$}
        \STATE $R_i = J(\{S{(k)} + f_i\})$ \underline{using Properties \ref{eq:update-inv}, \ref{eq:update-quad} and \ref{eq:update-log}}
        \ENDFOR
        \STATE $j=\text{arg} \max_{i} R_i$
        \IF{$R_j \geq J(S^{(k+1)})$}
        \STATE $k=k+1$
        \ELSE
        \STATE $S^{(k+1)} = \{S{(k)} + f_i\}$
        \STATE $k=k+1$
        \STATE $\text{flag}=1$
        \WHILE{$k > 2 \text{ and } \text{flag}=1$}
        \STATE \underline{Diagonalize $\boldsymbol{\Sigma}^{(k-1)} = = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^t$}
        \STATE \underline{\bfseries{for all} $\lambda_i$ \bfseries{do} $\lambda_i = \min (\text{EPS\_FLT},\lambda_i)$}
        \STATE \underline{Precompute $(\boldsymbol{\Sigma}^{(k-1)})^{-1}$, $(\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} (\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})$ and $\log \left(|\boldsymbol{\Sigma}^{(k-1)}|\right)$}
        \FORALL{$f_i \in S^{(k)}$}
        \STATE \underline{Compute update constant $\alpha$}
        \STATE \underline{$\alpha = \min (\text{EPS\_FLT},\alpha)$}
        \STATE $R_i = J(\{S{(k)}\setminus f_i\})$ \underline{using Properties \ref{eq:update-inv}, \ref{eq:update-quad} and \ref{eq:update-log}}
        \ENDFOR
        \STATE $j=\text{arg} \max_{i} R_i$
        \IF{$R_j < J(S^{(k-1)})$}
        \STATE $S^{(k-1)} = \{S{(k)} \setminus f_i\}$
        \STATE $k=k-1$
        \ELSE
        \STATE $\text{flag}=0$
        \ENDIF
        \ENDWHILE
        \ENDIF
        \ENDWHILE
        \RETURN $S$
        \end{algorithmic}
        }
        \end{algorithm}

    \section{Experimental results about features selection with GMM}

    In this section, we discuss the results of the features selection method. The forward selection and the floating forward selection are compared and the results with the different criterion function are also discussed. All the experimentation has been conducted with a dataset presented in the next subsection.

        \subsection{Aisa dataset}
        The dataset has been acquired by the AISA Eagle sensor during a flight campaign over Heves, Hungary. It contains 252 bands ranging from 395 to 975 nm. 16 classes have been defined for a total of 361,971 referenced pixels. Figure~\ref{fig:aisa} shows one channel of the multispectral image and the shapefile representing groundtruth and Table \ref{tab:aisa} presents the repartition of the various classes.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.5\textwidth]{Fig/aisa.png} &
                \includegraphics[width=0.5\textwidth]{Fig/aisa_gt.png} \\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \end{tabular}
            \caption{Aisa dataset: {\bfseries{(a)}} a single band of the image, {\bfseries{(b)}} groundtruth.\label{fig:aisa}}
        \end{figure}

        \begin{figure}[!ht]
            \centering
            \begin{tabular}[b]{lc}\hline
              Class & Number of samples \\ \hline
              Winter wheat & 136,524 \\
              Sunflower & 61,517 \\
              Green fallow last year treatment & 30,197 \\
              Alfalfa & 17,626 \\
              Maize & 18,278 \\
              Millet & 7,199 \\
              Broadleaved forest & 10,746 \\
              Meadow & 23,283 \\
              Winter barley & 2,799 \\
              Reed & 4,222 \\
              Water course & 4,773 \\
              Rape & 26,566 \\
              Green fallow with shrub & 9,272 \\
              Green fallow last year treated & 3,426 \\
              Pasture & 2,107 \\
              Oat & 3,436 \\ \hline
            \end{tabular}
            \caption{table}{Repartition of classes in Aisa dataset.\label{tab:aisa}}
        \end{figure}

        \subsection{Experimental results}

        For the results discussed in this section, the selection process was repeated 20 times with each time a different training set and the classification rate and time processing presented hereafter are the means over these 20 trials.

        % discuss evolution of classif rate with nb of selected var
        Figure~\ref{fig:res-1} corresponds to the evolution of the classification rate expressed with Cohen's kappa in function of the number of selected variables. We observe that at first the classification rate rapidly increase before stabilizing and then slowly decrease. This is the typical behavior expected when using features selection and a GMM classifier and it illustrated the Hughes phenomenon \cite{hughes1968mean} which states that with a given number of samples, prediction accuracy first increases with dimensionality but then decays when the complexity is higher than some optimum value. We want to use the number of variables which maximizes the classification rate even if the maximum is not always easily found for example because of local maxima.


        \begin{figure}[!ht]
            \centering
            \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_JM_fctvarnb.pdf}
            \caption{Classification results averaged on 20 trials using forward selection and Jeffries-Matusita distance as criterion with 100 samples by class in training set.\label{fig:res-1}}
        \end{figure}


        % discuss evolution with training set size
        It is also interesting to confirm the influence of the size of the training set. From Figure~\ref{fig:setsize}, we see that, either with an equalized training set or with a proportional training set, the classification rate is obviously better with more samples and also that the maximum is reached with more selected variables which that more samples allows to evaluate more precisely the relevant variables.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_setsize_1.pdf} &
                \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_setsize_2.pdf}\\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \end{tabular}
            \caption{Classification results averaged on 20 trials using forward selection and Jeffries-Matusita distance as criterion: {\bfseries (a)} with 50, 100, 200 samples by class in training set and {\bfseries (b)} with 0.5\%, 1\%, 2.5\% of dataset in training set keeping proportion.\label{fig:setsize}}
        \end{figure}

        % discuss sfs vs sffs
        Using Figure~\ref{fig:sfs-vs-sffs}, we can compare the results when using forward selection or SFFS. And we see that with this particular dataset, the SFFS does not produce better results than the other method. One should be careful not to draw conclusion too fast. It does not mean that the SFFS method has no advantages but rather that the SFFS advantages are highly related to the dataset structure and thus it is always good to check if the basic forward method is sufficient for a given dataset.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_fctmethod_1.pdf} &
                \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_fctmethod_2.pdf} \\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \end{tabular}
            \caption{Classification results with 20 trials : {\bfseries (a)} using Jeffries-Matusita distance as criterion and {\bfseries (b)} using Cohen's kappa as criterion.\label{fig:sfs-vs-sffs}}
        \end{figure}

        % discuss criterion advantages
        Now, if we take a closer to the influence of criterion function, we can see from Figure~\ref{fig:res-crit} the results of classification applied to the validation set for each of criterion presented in Section \ref{sec:criterion}. What we expected was that the criterion based on good classification mesure would be better because it would optimize directly the classification rate and maybe more flexible regarding to the Gaussian assumption. But it appears that for this given dataset, as good results are obtained with Jeffries-Matusita distance.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_allcrit_1.pdf} &
                \includegraphics[width=0.5\textwidth]{Fig/aisa_trials_20_allcrit_2.pdf} \\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \end{tabular}
            \caption{Classification results with different criterion function with 20 trials : {\bfseries (a)} with a training composed of 100 samples for each class and {\bfseries (b)} with a training set keeping class proportions composed of 1\% of the dataset (=xxxx samples).\label{fig:res-crit}}
        \end{figure}

        % discuss time processing
        Finally, if we compare the various method and criterion function in term of computational time, we first notice that the SFFS method takes more time than the forward method which is actually a fact supported by the theoretical ground. It can also be noticed than the processing time for selection do not increase with the number of samples in training set for metrics based on distance between distribution (Kullback–Leibler divergence, Jeffries-Matusita distance) whereas, in the case of good classification criteria, the processing time rapidly increases. All processing time are presented in Figure~\ref{fig:proc-time}.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.4\textwidth]{Fig/time_aisa_trials_20_kappa.pdf} &
                \includegraphics[width=0.4\textwidth]{Fig/time_aisa_trials_20_accuracy.pdf} \\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
                \includegraphics[width=0.4\textwidth]{Fig/time_aisa_trials_20_F1Mean.pdf} &
                \includegraphics[width=0.4\textwidth]{Fig/time_aisa_trials_20_divKL.pdf} \\
                {\bfseries{(c)}} & {\bfseries{(d)}} \\
                \includegraphics[width=0.4\textwidth]{Fig/time_aisa_trials_20_JM.pdf} & \\
                {\bfseries{(e)}} & \\
            \end{tabular}
            \caption{(Figures temporaires en attente de resultats) Processing time for selection with 20 trials : {\bfseries (a)} using Cohen's kappa, {\bfseries (b)} using overall accuracy, {\bfseries (c)} using mean of F1-scores, {\bfseries (d)} using KL divergence, {\bfseries (e)} using JM distance.\label{fig:proc-time}}
        \end{figure}




    \section{Implementation of the Orfeo Toolbox module}

    After the calibration of the algorithm with a Python implementation, we aim to realize a C++ implementation of this selection method. And, to make it easily available, we choose to develop a remote module for the Orfeo Toolbox (OTB) designed by the CNES. The current section describes this C++ implementation.

        \subsection{Integration in Orfeo Toolbox}

        The Orfeo Toolbox is an open-source library for remote sensing image processing. This library allows developers to code external modules and make them available to the community. It is to notice that the toolbox already implements several classifiers inherited mostly from OpenCV. Nevertheless, the GMM classifier currently in place is not robust to high dimensionality (see Section~\ref{sec:exp-otb}) and that's why we choose to implement our own model.

        The module we developed is available on Github \footnote{\url{https://github.com/Laadr/otbExternalFastFeaturesSelection}} and is actually a fork of the template for remote module furnished by OTB developers.

        \subsection{Code structure}

        \begin{figure}[!ht]
            \centering
            \begin{tikzpicture}
                \begin{umlpackage}{otb}
                    \umlclass[template={InType,LabType}, x=4.5]{MachineLearningModel}{
                        m\_InputListSample : ListSample<InType>*\\
                        m\_TargetListSample : ListSample<LabType>*
                    }{
                        \umlvirt{Train() : void} \\
                        \umlvirt{Predict() : LabType} \\
                        \umlvirt{Save() : void} \\
                        \umlvirt{Load() : void} \\
                        \umlvirt{CanReadFile() : bool} \\
                        \umlvirt{CanWriteFile() : bool}
                    }
                \end{umlpackage}

                \begin{umlpackage}{otbExternalFastFeaturesSelection}
                    \umlclass[template={InType,LabType}, y=-8]{GMMMachineLearningModel}{
                        m\_ClassNb : unsigned\\
                        m\_FeatNb : unsigned\\
                        m\_Proportion : vector<double>\\
                        m\_NbSpl : vector<unsigned>\\
                        m\_Means : vector<vnl\_vector>\\
                        m\_Covariances : vector<vnl\_matrix>\\
                        m\_Tau : RealType\\
                        m\_RateGridsearch : vector<RealType>\\
                        m\_EigenValues : vector<vnl\_vector>\\
                        m\_Q : vector<vnl\_matrix>\\
                        m\_LambdaQ : vector<vnl\_matrix>\\
                        m\_CstDecision : vector<RealType>
                    }{
                        SetTau() : void\\
                        TrainTau() : void\\
                        Decomposition() : void\\
                        Train() : void\\
                        Predict() : LabType\\
                        Save() : void\\
                        Load() : void\\
                        CanReadFile() : bool\\
                        CanWriteFile() : bool
                    }
                    \umlclass[template={InType,LabType}, x=8.5, y=-8]{GMMSelectionMachineLearningModel}{
                        m\_VarNbPrediction : int \\
                        m\_EnableOptimalSet : bool \\
                        m\_SelectedVar : vector<int>\\
                        m\_BestSets : vector<vector<int>> \\
                        m\_CriterionBestValues : vector<RealType> \\
                        m\_SubMeans :  vector<vnl\_vector> \\
                        m\_Logprop : vector<RealType> \\
                        m\_SubmodelCv : vector<ModelType> \\
                        m\_Fold : vector<ListSample*>
                    }{
                        Selection() : void\\
                        ForwardSelection() : void\\
                        FloatingForwardSelection() : void\\
                        ComputeClassifRate() : void\\
                        ComputeJM() : void\\
                        ComputeDivKL() : void\\
                        Train() : void\\
                        Predict() : LabType\\
                        Save() : void\\
                        Load() : void\\
                    }
                \end{umlpackage}

                \umlinherit[geometry=-|,anchor1=10]{GMMMachineLearningModel}{MachineLearningModel}
                \umlinherit[geometry=|-]{GMMSelectionMachineLearningModel}{GMMMachineLearningModel}
            \end{tikzpicture}
            \caption{UML digram of the developped OTB remote module.\label{fig:uml}}
        \end{figure}

        Even if a GMM classifier inherited from Opencv library is already available in the OTB, we choose to implement our own version of a GMM classifier in a class named \emph{GMMMachineLearningModel} in order to assure better performance in term of computational time. The second step is to implement a subclass of the GMM classifier including the features selection method.

        The \emph{GMMMachineLearningModel} class inherits from the OTB class \emph{MachineLearningModel} which is a basic class used for all classifier in the OTB. The \emph{MachineLearningModel} class enables the management of a list of samples used for training and also declares the virtual methods inherited: \emph{Train()}, \emph{Predict()}, \emph{Save()}, \emph{Load()}, \emph{CanReadFile()}, \emph{CanWriteFile()}.

        The \emph{GMMMachineLearningModel} class implements all these virtual methods. The \emph{Train()} aims to train the classifier using a list of samples set as a member of the class. Then, it is possible to use the method \emph{Predict()} to classify a sample and optional get the confidence in the prediction. The methods \emph{Save()} and \emph{Load()} obviously are used to save and load a trained model. And, finally, \emph{CanReadFile()} and \emph{CanWriteFile()} state the validity of a GMM model file.

        In addition to these inherited methods, two noticeable methods are implemented. The \emph{Decomposition()} method perform a decomposition of a symmetric matrix in eigenvalues and eigenvectors using a function of the VNL library. It is to notice that this method check the value of the extracted eigenvalues and minor them to EPSILON\_FLT (or EPSILON\_DBL) which corresponds to computational precision.  The idea is to limit computational errors when parameters are ill-estimated due to an insufficient amount of training samples. The second interesting method is \emph{TrainTau()} which concerns the Ridge regularization. Given a set of possible regularization constant $\tau$, this method selects the most efficient value in estimating classification rate with cross-validation.

        In term of members, the \emph{GMMMachineLearningModel} includes:
        \begin{itemize}
            \item 6 variables describing the model: the number of class, the number of features, the proportion of each class in the training set, the number of samples in each class, the mean vectors of each class and finally the covariance matrices of each class;
            \item 1 optional meta-parameter: the regularization value $\tau$ for Ridge regularization;
            \item 1 variable the result of tau selection;
            \item 4 precomputed terms used for prediction: eigenvalues of covariance matrices, eigenvectors of covariance matrices, 2 other terms.
        \end{itemize}

        To implement the selection algorithm, a new class \emph{GMMSelectionMachineLearningModel} inheriting from \emph{GMMMachineLearningModel} is defined. This class keeps the same six methods inherited from \emph{MachineLearningModel} but reimplements \emph{Predict()} to predict with a reduced set of features and  \emph{Save()}/\emph{Load()} to use a second file to handle the results of the selection.

        The other methods available in class \emph{GMMSelectionMachineLearningModel} are used to perform the selection algorithms presented in Section \ref{sec:theory}. The method \emph{Selection()} aims to set the various parameter of the selection and then calls either the \emph{ForwardSelection()} method to use forward feature selection or the \emph{FloatingForwardSelection()} method to use SFFS. Then, 3 different methods are used to evaluate criterion functions during selection. Given a pool of variables available for selection, these functions evaluate the criterion functions for all the possible augmented set. The \emph{ComputeJM()} method is used for Jeffries-Matusita distance, \emph{ComputeDivKL()} for Kullback–Leibler divergence and \emph{ComputeClassifRate()} for the three good classification criteria. It is to notice that the good classification criteria are evaluated using cross-validation.

        Several new members are defined in the \emph{GMMSelectionMachineLearningModel} class:
        \begin{itemize}
            \item 2 meta-parameters: one to specify the number of variables to use for prediction and one to enable the process to automatically set the previous meta-parameter to the number maximizing the criterion function;
            \item 3 variables describing the results of selection: the set to use for prediction, the evolution of criterion function at each iteration and the best sets for each iteration;
            \item 2 variables used for cross-validation: the set of submodels and the folds of samples;
            \item 2 precomputed terms used for prediction.
        \end{itemize}

        All the code structure is summarized in the UML diagram displayed in Figure~\ref{fig:uml}.

        \subsection{Applications}

        In order to provide an easy way to use the developed algorithm, we finally develop OTB application. These application allows the user to use and parametrize the algorithm from command line. Three applications are built.

        The application \emph{otbcli\_TrainGMMApp} create and train a model from class \emph{GMMMachineLearningModel} and the application \emph{otbcli\_TrainGMMSelectionApp} same for a model of class \emph{GMMSelectionMachineLearningModel}. These two applications use a raster image and shapefile with groundtruth as input and create a model file.

        The last application \emph{otbcli\_PredictGMMApp} takes a raster image as input and a model file and perform classification of the image. It generates a classification map in an image file and optionally a confidence map. An other application is available in the OTB to compute classification rate from the image and a groundtruth file.

        \subsection{Code profiling}

        A voir, si je trouve un truc lisible a presenter mais ca serait bien...


    \section{Experimental results comparison to classical classifiers}
    \label{sec:exp-otb}



    \section{Conclusion}



\newpage

\appendix
\section{Update for cross validation (calculation)}
\label{app:cv_maj}

    \subsection{Mean update}
        \begin{align*}
            \boldsymbol{\mu}_c^{n_c} &= \frac{1}{n_c} \sum_{j = 1}^{n_c} \mathbf{x}_j \\
                        &= \frac{1}{n_c} \sum_{j = 1}^{n_c-\nu_c} \mathbf{x}_j + \frac{1}{n_c} \sum_{j = n_c-\nu_c +1}^{n_c} \mathbf{x}_j \\
                        &= \frac{n_c-\nu_c}{n_c} \boldsymbol{\mu}_c^{n_c-\nu_c} + \frac{\nu_c}{n_c} \boldsymbol{\mu}_c^{\nu_c} \\
                        &= \boldsymbol{\mu}_c^{n_c-\nu_c} + \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})
        \end{align*}

        \begin{equation}
             \fbox{$\boldsymbol{\mu}_c^{n_c} = \boldsymbol{\mu}_c^{n_c-\nu_c} + \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})$}
        \end{equation}

        \begin{equation}
            \fbox{$\boldsymbol{\mu}_c^{n_c-\nu_c} = \frac{n_c \boldsymbol{\mu}_c^{n_c} - \nu_c \boldsymbol{\mu}_c^{\nu_c}}{n_c - \nu_c}$}
        \end{equation}

    \subsection{Covariance matrix update}
        \begin{align*}
            \boldsymbol{\Sigma}_c^{n_c} &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c})^t \\
                           &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c} - \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c} - \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c}))^t \\
                           &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                                                                 &\qquad + \frac{\nu_c^2}{n_c^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                                                                 &\qquad - \frac{\nu_c}{n_c} (\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                                                                 &\qquad - \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})^t
        \end{align*}

        \begin{itemize}
            \item $(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t = \frac{n_c^2}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$
            \item $\frac{1}{n_c} \sum_{j = 1}^{n_c} (\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t = \frac{n_c \nu_c}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$
            \item $\frac{1}{n_c} \sum_{j = 1}^{n_c} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})^t = \frac{n_c \nu_c}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$
        \end{itemize}

        \begin{align*}
            \boldsymbol{\Sigma}_c^{n_c} &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t - \frac{\nu_c^2}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
                           &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c-\nu_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t + \frac{1}{n_c - 1} \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                           &\qquad - \frac{n_c \nu_c^2}{(n_c-1)(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
                           &= \frac{n_c-\nu_c-1}{n_c-1} \boldsymbol{\Sigma}_c^{n_c-\nu_c} + \frac{1}{n_c - 1} \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                           &\qquad - \frac{n_c \nu_c^2}{(n_c-1)(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
        \end{align*}

        \begin{align*}
            \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t
                &= \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c} + \boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c} + \boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})^t\\
                &= (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} + \nu_c (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t\\
                &\qquad + \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c}) (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})^t\\
                &\qquad + \sum_{j = n_c-\nu_c+1}^{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c})^t\\
                &= (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} + \frac{n_c^2 \nu_c}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
        \end{align*}

        \begin{equation}
            \fbox{$\boldsymbol{\Sigma}_c^{n_c} = \frac{n_c-\nu_c-1}{n_c-1} \boldsymbol{\Sigma}_c^{n_c-\nu_c} + \frac{\nu_c-1}{n_c-1} \boldsymbol{\Sigma}_c^{\nu_c} + \frac{n_c \nu_c}{(n_c-1)(n_c-\nu_c)} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$}
        \end{equation}

        \begin{equation}
            \fbox{$\boldsymbol{\Sigma}_c^{n_c-\nu_c} = \frac{1}{n_c-\nu_c-1} ( (n_c-1) \boldsymbol{\Sigma}_c^{n_c} - (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} - \frac{n_c \nu_c}{(n_c-\nu_c)} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$ )}
        \end{equation}


\bibliographystyle{ieeetr}
\bibliography{biblio}

\end{document}
