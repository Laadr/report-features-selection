\documentclass[a4paper,11pt,DIV=16]{scrartcl}
\usepackage{ucs}
\usepackage{array}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[normalem]{ulem}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}\hypersetup{colorlinks=true,hypertexnames=false}
\usepackage[osf,sc]{mathpazo}
\usepackage{booktabs}
\usepackage{graphicx}

\usepackage[usenames,dvipsnames]{xcolor}\definecolor{bg}{rgb}{0.95,0.95,0.95}
\usepackage[english]{babel}
\usepackage{minted}\usemintedstyle{emacs}
\usepackage{listings}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfgantt}

\title{Report : Operational Feature Selection in Gaussian Mixture Models}


\author{Adrien Lagrange
\\
\small{Dynafor}}

\date{\today}

\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

\begin{document}
\maketitle

\section{Introduction}

With the increasing number of remote sensing missions, the quantity of data available for a given landscape becomes larger and larger. In particular, the European satellite Sentinel-2 was launched successfully recently and the hyperspectral and hypertemporal data produced by this mission will be fully available at the end of 2016\footnote{\url{https://sentinel2.cnes.fr/en/sentinel-2-0}}. Therefore, data is more and more difficult to process because computational and also statistical limits. Hence, in many remote sensing applications, the extraction of features from a large amount of available data is required.

For instance, in land-cover classification, given a set of spatial, temporal and spectral features, it is possible to extract those which are the most discriminant for the purpose of classification as explained in \cite{Guyon:2006:FEF:1208773}. In hyperspectral data analysis from the hundreds of available spectral channels, it is possible to reduce the number of channels to make the processing more efficient.

There are two ways to reduce dimension: features extraction and feature selection. Feature extraction means creating new features in combining the existing ones, for example linear combination as in Principal Component Analysis \cite{jimenez1998supervised}. To the contrary, features selection select a subset of the existing features which has the advantage to be much more interpretable for the end-user. The selected subset of features corresponds to the most important features for the given task.

There is a large diversity of method for feature selection. However, they usually do not scale well with the number of pixels to be processed as shown in \cite{fauvel2015fast}. Nevertheless, methods based on Gaussian Mixture Models (GMM) have several interesting properties described in \cite{webb2003statistical} that make them suitable for feature selection in the context of large amount of data. By taking advantage of their intrinsic properties, it is possible to increase the computational efficiency with respect to standard implementation.

The objectives of the internship is to develop and implement a feature selection method based on GMM. The remaining of this mid-term report is organized as follows.

\section{Non linear parsimonious features selection}

In the remaining, the following notations are used. $\mathcal{S} = \{\mathbf{x}_i,y_i\}_{i=1}^{n}$ is the training set where $\mathbf{x}_i \in \mathbb{R}^d$ is the vector of features of the $i^{th}$ sample, $y_i = 1,...,C$ the associated label, C the number of labels, $n$ the number of samples and $n_c$ the number of samples of class $c$.

    \subsection{Gaussian Mixture Models}

    The hypothesis of Gaussian mixture models (GMM) is that the distribution of a given sample is a mixture (convex combination) of several class conditioned distribution:
    \begin{equation}
        p(\mathbf{x}) = \sum_{c=1}^{C} \pi_c f_c(\mathbf{x}|\theta)
    \end{equation}
    where $\pi_c$ is the prior i.e. the proportion of class $c$.
    The Gaussian model assumes that each $f_c$ is , conditionally to $c$, a Gaussian distribution of parameters $\boldsymbol{\mu}_c$ and $\boldsymbol{\Sigma}_c$ ($f_c(\mathbf{x}|\theta) = \mathcal{N}_c(\boldsymbol{\mu}_c, \boldsymbol{\Sigma}_c)$) and so $f_c(\mathbf{x}|\theta)$ can be written
    \begin{equation*}
        f_c(\mathbf{x}|\theta) = \frac{1}{(2\pi)^{\frac{d}{2}} |\boldsymbol{\Sigma}_c|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right)
    \end{equation*}

    Such a model is used in the case of supervised learning and the class parameters $\boldsymbol{\mu}_c$ and $\boldsymbol{\Sigma}_c$ can be estimated using the training samples. In our work, we choose to compute them with the following unbiased empirical estimator
    \begin{align}
        \hat{\pi}_c &= \frac{n_c}{n}\\
        \hat{\boldsymbol{\mu}}_c &= \frac{1}{n_c} \sum_{\{i|y_i = c\}} \mathbf{x}_i \\
        \hat{\boldsymbol{\Sigma}}_c &= \frac{1}{(n_c - 1)} \sum_{\{i|y_i = c\}} (\mathbf{x}_i - \boldsymbol{\mu}_c) (\boldsymbol{}x_i - \boldsymbol{\mu}_c)^t
    \end{align}

    To determine the class of a samples using the Bayes' law, the maximum a posteriori (MAP) estimate is used and thus the decision rule can be written as
    \begin{align*}
        \mathbf{x} \text{ belongs to } c &\Leftrightarrow c = \text{arg} \max_{c \in C} p(c|\mathbf{x}) \\
                                         &\Leftrightarrow c = \text{arg} \max_{c \in C} \frac{p(c) p(\mathbf{x}|c)}{p(x)} \\
                                         &\Leftrightarrow c = \text{arg} \max_{c \in C} p(c) p(\mathbf{x}|c) \\
    \end{align*}
    By taking the log, a simplified decision formula is obtained
    \begin{align}
        Q_c(\mathbf{x})
        &= 2 \log \left( p(c) p(\mathbf{x}|c) \right) \nonumber \\
        &= 2 \log \left( \pi_c \frac{1}{(2\pi)^{\frac{d}{2}} |\boldsymbol{\Sigma}_c|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right) \right) \nonumber \\
        &= 2 \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right) + 2 \log \left( \frac{1}{(2\pi)^{\frac{d}{2}} |\boldsymbol{\Sigma}_c|^{\frac{1}{2}}} \right) + 2 \log (\pi_c) \nonumber \\
        &= - (\mathbf{x} - \boldsymbol{\mu}_c)^t \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) - \log ((2\pi)^d|\boldsymbol{\Sigma}_c|) + 2 \log (\pi_c)
        \label{eq:decision}
    \end{align}

    It is interesting to notice the covariance and its inverse are a key element of the decision function and that the estimation of these elements suffer from the curse of dimensionality. More precisely, the number of parameters to estimate (means, covariance matrices, proportions) increases quadratically relatively to the number of features and we need at least as many samples as parameters to make an estimation which can be an issue. For example, with hyperspectral data, we face high dimensional samples but very few samples are available.

    A lack of samples induces ill-conditioned covariance matrices and so unstable inversion. There are two major solutions to this problem. The first option is to use a regularization method to stabilize the inversion of the covariance matrices. The second option is to use a features extraction method in order to reduce the dimensionality of the samples.

    In our study based on GMM, a Ridge regularization method described in Section \ref{sec:regularization} is implemented but we mainly focus on a feature selection method named sequential forward features selection presented in Section \ref{sec:forward-presentation} and an amelioration of this algorithm the sequential floating forward features selection introduced in Section \ref{sec:floating-presentation}.

    In order to evaluate the benefits of such methods for classification, we introduce in the next section several functions called criterion functions. These functions aim either to evaluate a criterion of good classification or either to evaluate the separability/similarity of class distributions. The choice of one of these criterion functions is up to the user and can depend on the specificity of the task. We purposely choose to present only some of possible functions among the most used. It is also to be noticed that the criterion function is not only use for evaluation in the case of the features selection algorithm presented hereafter but is indeed a parameter of the algorithm.


    \subsection{Criterion function}
    \label{sec:criterion}

        \subsubsection{Measures of good classification}

        All this measures of good classification are based on an error matrix $M$ called confusion matrix which is defined so that $M_{ij}$ is the number of samples of class $i$ classified as class $j$. The confusion matrix allows the computation of several interesting values relatively to each class the number of True Positive (TP) corresponding to good prediction, True Negative (TN) corresponding to good classification of the other class, False Negative (FN) corresponding to the samples of the class labeled as an other class and the False Positive (FP) corresponding to the samples wrongly classified as part of of this class. Figure \ref{fig:confusion} illustrates the definition of this values.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
              \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ value}} &
                & \multicolumn{2}{c}{\bfseries Prediction outcome} \\
              & & \bfseries p & \bfseries n \\
              & p$'$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} \\[2.4em]
              & n$'$ & \MyBox{False}{Positive} & \MyBox{True}{Negative}
            \end{tabular}
            \caption{Confusion matrix}
            \label{fig:confusion}
        \end{figure}

        \paragraph{The overall accuracy} is simply the rate of the number of samples with the correct predicted label over the number of samples. This metric is easy to interpret but is biased in the case of unbalanced classes.

        \paragraph{The Cohen's kappa} is a statistic which measure the probabilities of agreement between predictions and ground-truth. This metric tends to give an equal importance to each class but is hard to interpret.

        \paragraph{The mean F1 score} is the average of the F1 score for each class and the F1 score is the harmonic mean of the precision (number of True Positive over True Positive plus False Positive) and the recall (number of True Positive over True Positive plus False Negative).

        \begin{align}
            \text{Overall Accuracy} &= \frac{\text{TP}}{n} \\
            \text{Kappa} &= \frac{pa - pr}{1 - pr}  \\
            \text{F1 Mean} &= \frac{2 \text{TP}}{2 \text{TP} + \text{FN} + \text{FP}}
        \end{align}
        where TP stands for True Positive, FN for False Negative, FP for False Positive, $pa$ is the probability of agreement defined by $pa = Overall Accuracy$  and $pr$ the probability of random agreement defined by $pr = \sum_{c=1}^{C} \frac{TP_c}{FP_c} \frac{TP_c}{FN_c}$.

        In order to compute these metrics, the predicted labels are needed and so we need to compute the decision function (Equation \ref{eq:decision}). Moreover to estimate this measures of good classification, a cross-validation method is used which reduces both bias and variance of the estimation.

        \subsubsection{Measures of similarity between distributions}

        \paragraph{The Kullback–Leibler divergence} is a measure of distance between two distributions. It actually measures the amount of information lost when the first distribution is approximated by the second one. The formal definition is
        \begin{equation}
            \text{Div}_{KL}(c_i,c_j) = \int_\mathbf{x} p(\mathbf{x}|c_i) \ln(\frac{p(\mathbf{x}|c_i)}{p(\mathbf{x}|c_j)}) d\mathbf{x}
        \end{equation}
        And in the case of Gaussian model, it can be rewritten as follows
        \begin{align}
            \text{Div}_{KL}(c_i,c_j) = \frac{1}{2} \left( \text{Tr} (\boldsymbol{\Sigma}_{c_i}^{-1} \boldsymbol{\Sigma}_{c_j}) + (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_i})^t \boldsymbol{\Sigma}_{c_i}^{-1} (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_i}) - d + \log \left( \frac{|\boldsymbol{\Sigma}_{c_i}|}{|\boldsymbol{\Sigma}_{c_j}|} \right) \right)
        \end{align}
        where Tr is the trace operator and $k$ the dimension of the distribution.

        It can be noticed that the KL divergence is not symmetric and so the symmetrized version is used to compute the criterion function. In the case of Gaussian model, the symmetrization induces the following simplification of the formula
        \begin{align}
            \text{SKL}_{ij} &=\text{Div}_{KL}(c_i,c_j) + \text{Div}_{KL}(c_j,c_i) \nonumber \\
            &= \frac{1}{2} \left( \text{Tr} (\boldsymbol{\Sigma}_{c_i}^{-1} \boldsymbol{\Sigma}_{c_j} + \boldsymbol{\Sigma}_{c_j}^{-1} \boldsymbol{\Sigma}_{c_i}) + (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_i})^t (\boldsymbol{\Sigma}_{c_i}^{-1} + \boldsymbol{\Sigma}_{c_j}^{-1}) (\boldsymbol{\mu}_{c_i} - \boldsymbol{\mu}_{c_i}) - 2d \right)
        \end{align}

        Moreover, the divergence is computed between two classes and to obtain a unique value the weighted mean of divergence measures is taken
        \begin{equation}
            \text{C}_{SKL} = \sum_{i=1}^{C} \sum_{j=i + 1}^{C} \pi_{c_i} \pi_{c_j} \text{SKL}_{ij}
        \end{equation}

        \paragraph{The Bhattacharyya distance} is an other measure of similarity between two distributions. In fact, we do not discuss the efficiency of this measure in our work and simply used it to compute the next measure. The Bhattacharyya distance is computed as follows
        \begin{equation}
            \text{B}_{ij} = - \ln \left( \int_\mathbf{x} \sqrt{p(\mathbf{x}|c_i) p(\mathbf{x}|c_j)} d\mathbf{x} \right)
        \end{equation}
        And in the case of Gaussian model:
        \begin{equation}
            \text{B}_{ij} = \frac{1}{8} (\boldsymbol{\mu}_i - \boldsymbol{\mu}_j)^t \left( \frac{\boldsymbol{\Sigma}_i + \boldsymbol{\Sigma}_j}{2} \right)^{-1} (\boldsymbol{\mu}_i - \boldsymbol{\mu}_j) + \frac{1}{2} \ln \left( \frac{|\boldsymbol{\Sigma}_i + \boldsymbol{\Sigma}_j|}{\sqrt{|\boldsymbol{\Sigma}_i| |\boldsymbol{\Sigma}_j|}} \right)
        \end{equation}

        \paragraph{The Jeffries–Matusita distance} is a measure based on the Bhattacharyya distance but transform in a way that the distance saturate if the separability between the two distribution increases. The JM distance is defined according to
        \begin{equation}
            \text{JM}_{ij} = \sqrt{ \int_\mathbf{x} \left[\sqrt{p(\mathbf{x}|c_i)} - \sqrt{p(\mathbf{x}|c_j)}\right]^2 d\mathbf{x} }
        \end{equation}
        And the Jeffries–Matusita distance can be rewritten according to the Bhattacharyya distance
        \begin{equation}
            \text{JM}_{ij} = \sqrt{ 2 \{1 - \text{exp}[-B_{ij}]\} }
        \end{equation}

        As for the KL divergence, a weighted mean of the distance between two classes is computed to aggregate the measures in a single value.

        \vspace{10 mm}

        According to \cite{bruzzone2009novel}, it is interesting to notice that the KL divergence increases quadratically with respect to the distance between the mean vectors of the class distributions whereas the measures of good classification we used asymptotically tends to one when distributions are perfectly separable. On the contrary, the JM distance tends to saturate as these measures of good classification.


\section{Ridge Regularization}
\label{sec:regularization}

    \subsection{Principe of Ridge Regularization}
    As introduced in the previous section, the aim of a regularization method is to stabilize the inversion of the covariance matrix in the context of high dimensionality which often imply a ill-conditioned matrix. In particular, the Ridge regularization proposes to solve the problem in adding a small value $\tau$ to each eigenvalues of the covariance matrix. In our case, we choose to add the same constant to each eigenvalues but it is not compulsory in a general case.

    Thus, to perform the regularization, the covariance matrix is decomposed with a diagonalization algorithm for symmetric matrices s.t. $\boldsymbol{\Sigma} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^t$ where $\mathbf{Q}$ is the matrix of eigenvectors and $\boldsymbol{\Lambda}$ is the diagonal matrix of eigenvalues. When the decomposition is available, the regularization almost finish and the decision function is simply rewritten as follow

    \begin{align}
    \label{eq:decision-ridge}
        Q_c(\mathbf{x}) = - (\mathbf{x} - \boldsymbol{\mu}_c)^t \mathbf{Q}_c (\boldsymbol{\Lambda}_c + \tau \mathbf{I})^{-1} \mathbf{Q}_c^t (\mathbf{x} - \boldsymbol{\mu}_c) - \log ((2\pi)^d|\boldsymbol{\Sigma}_c + \tau \mathbf{I}|) + 2 \log (\pi_c)
    \end{align}
    where $\mathbf{I}$ is the identity matrix. It is to be noticed that $(\boldsymbol{\Lambda} + \tau \mathbf{I})$ is also a diagonal matrix and so the inverse is easily obtained in inverting each element of the matrix.

    \subsection{Implementation}

    The main steps of the training algorithm when using Ridge regularization are summarized in Algorithm \ref{alg:ridge}. Indeed, before training the GMM model, a preliminary step is needed in order to determine a good value for the parameter $\tau$ and to do so, a gridsearch is performed with a set of values $\mathcal{P}$ given by the user. Moreover, to have a good estimation of the quality of the classification with a given $\tau$ a cross-validation method is used. Thus, the training set is divided in k folds, then, the training is done with $(k-1)$ folds and the performances are estimated with the remaining fold.

    A particularity of this regularization method is used to accelerate the gridsearch. More precisely, the two computationally costly step are the computation of the inverse and the determinant of the regularized covariance matrices and it is important to see that these operations can be performed only once to test all $\tau$. The covariance matrix is diagonalized before selecting $\tau$ and the eigenvalues and eigenvectors are stored and used to test all $\tau$ using Equation \ref{eq:decision-ridge}.

    \begin{algorithm}
    \caption{Ridge Regularization training steps\label{alg:ridge}}
    \begin{algorithmic}[1]
    \REQUIRE $\mathcal{S},k,\mathcal{P}$
    \STATE Randomly cut $\mathcal{S}$ in $k$ subsets such as $\mathcal{S}_1 \cup \text{...} \cup \mathcal{S}_k = \mathcal{S}$ and $\mathcal{S}_i \cap \mathcal{S}_j = \emptyset$
    \FORALL{$\mathcal{S}_i$}
    \STATE Learn GMM with $\mathcal{S} \setminus \mathcal{S}_i$
    \STATE Compute decomposition of covariance matrices of each classes s.t. $\boldsymbol{\Sigma}_c = \mathbf{Q}_c \boldsymbol{\Lambda}_c \mathbf{Q}_c^t$
    \FORALL{$\tau \in \mathcal{P}$}
    \STATE Estimate classification rate on $\mathcal{S}_i$ using Equation \ref{eq:decision-ridge}
    \ENDFOR
    \ENDFOR
    \STATE Average the classification rate over the $k$ folds
    \STATE Compute $\text{arg} \max_{\tau}$ of the classification rates to get the best parameter $\tau^*$
    \STATE Learn GMM with the whole training set $\mathcal{S}$ and parametrize the GMM with $\tau^*$
    \end{algorithmic}
    \end{algorithm}

    Finally, another trick could used to optimize the gridsearch. As explained in the next section, it is possible to avoid the learning step on the $(k-1)$ fold and instead learn the model with the whole dataset and deduce the submodel used for cross-validation from the complete model and the mean vector and covariance matrix of the $k^{th}$ fold.

    As stated before, we choose to implement a basic Ridge regression method and only the small improvement explained herebefore as been proposed. To the contrary, we choose in our study to focus more on a feature selection method named sequential forward features selection described in following section.

\section{Features selection}

We recall that the aim of feature selection method is to select a subset of variables to described each samples and so to get rid of high dimensionality and its curse. It is also important to underline that the proposed method is a \emph{selection} method and not an \emph{extraction} method. It means the subset of variables obtained at the end is composed of actual variables of the original set and not variables built as combination of several others as it is the case for example with PCA. This choice is made to assure an easier interpretation by the user of the obtained subset of variables.

Two selection algorithms are presented the sequential forward features selection method (Section \ref{sec:forward-presentation}) and the sequential floating forward feature selection method (\ref{sec:floating-presentation}) the second being a more complex variation of the first.

    \subsection{Sequential forward features selection}
    \label{sec:forward-presentation}

    The Sequential Forward Selection (SFS) starts with an empty set of selected features. Then it tests at each step for all the remaining features the value of a criterion function $J$ chosen by the user among the ones presented in Section \ref{sec:criterion} when the feature is added to the pool of selected features. The feature that maximizes the criterion function is definitively added to the pool of selected features and it moves to the next iteration. The algorithm stops when a given number of variables \emph{maxVar} has been selected.

    To summarize, the features are selected one by one and the set of selected features are never put into questions. It results in a reasonable computational time but a suboptimal solution to the selection problem.

    \begin{algorithm}
    \caption{Sequential forward features selection\label{alg:sfs}}
    \begin{algorithmic}[1]
    \REQUIRE $\mathcal{S},J,\text{maxVar}$
    \STATE $S=\emptyset$
    \STATE $F=\text{\{all variables\}}$
    \WHILE{Repeat till $S$ contained $\text{maxVar}$ variables}
    \FORALL{$f_i \in F$}
    \STATE $R_i = J(\{S + f_i\})$
    \ENDFOR
    \STATE $j=\text{arg} \max_{i} R_i$
    \STATE $S = \{S + f_j\}$
    \STATE $F = F \setminus f_j$
    \ENDWHILE
    \RETURN $S$
    \end{algorithmic}
    \end{algorithm}


    Figure \ref{fig:expl_dataset} shows a toy dataset with 2 classes and 2 features. The projection of the Gaussian distribution learned for each class can be visualized on the corresponding axis. In this examples, each class has been generated given a Gaussian law but it is not necessary in real case.

    \begin{figure}[!ht]
        \centering
        \begin{tabular}{cc}
            \includegraphics[width=0.5\textwidth]{Fig/example_case1.pdf} &
            \includegraphics[width=0.5\textwidth]{Fig/example_case2.pdf} \\
            {\bfseries{(a)}} & {\bfseries{(b)}} \\
        \end{tabular}
        \caption{Example datasets: {\bfseries{(a)}} 2 classes and 2 features but only one informative feature, {\bfseries{(b)}} same dataset with a reduced gap between means of informative feature.\label{fig:expl_dataset}}
    \end{figure}

    In the case of the example presented in Figure \ref{fig:expl_dataset}, we compute the various criterion functions using a single feature and summarize it in Table \ref{tab:expl_metric}. It corresponds to first step of the SFS. And we can see that all the criterion functions are maximum with the feature 1 which makes sense because the projection of the learned Gaussian distribution effectively seems separable only for feature 1. So the algorithm will has expected select the feature 1 as the best features to perform classification.

    \begin{table}[!ht]
        \centering
        \begin{tabular}{l|r|r|r|r}
             & \multicolumn{2}{c|}{Case a} & \multicolumn{2}{c}{Case b} \\
            \hline
             & Feat 1 & Feat 2 & Feat 1 & Feat 2 \\
            \hline
            Overall Accuracy          & 0.995 & 0.445 & 0.935 & 0.445  \\
            Cohen's kappa             & 0.99  & -0.11 & 0.87  & -0.11 \\
            F1-score mean             & 0.995 & 0.43  & 0.935 & 0.43 \\
            KL divergence             & 13.04 & 0.25  & 3.48  & 0.25 \\
            Jeffrey-Matusita distance & 0.35  & 0.19  & 0.31  & 0.19 \\
        \end{tabular}
        \caption{Criterion functions values corresponding to Figure \ref{fig:expl_dataset} dataset (50\% for training/50\% for testing).\label{tab:expl_metric}}
    \end{table}


    \subsection{Sequential floating forward feature selection}
    \label{sec:floating-presentation}

    The Sequential Floating Forward Selection (SFFS) is actually based on two algorithms: the SFS described above and the Sequential Backward Selection (SBS). The SBS is the backward equivalent of SFS. The difference is that it starts with every features in the pool of selected features and tries at each step to remove the less significant one in term of the given criterion function.

    The SFFS works as the SFS but between each step of the SFS algorithm a backward selection is operated. At the end of the SBS step, the value of the criterion function is compared to the best value ever obtained with a set of features of the same size and if the new value is better the feature puts into question is effectively taken away and the next step is again a SBS but if the new value is not better the SBS step is forgotten and it moves to the next SFS step. The algorithm stops when a given number of features has been selected.

    This SFFS algorithm eventually tests more solutions than the SFS algorithm. The results are expected to be better but the trade-off is an increased computational time which is dependent of the complexity of the dataset.

    \begin{algorithm}
    \caption{Sequential floating forward features selection\label{alg:sffs}}
    \begin{algorithmic}[1]
    \REQUIRE $\mathcal{S},J,\text{maxVar}$
    \STATE $S=\emptyset$
    \STATE $F=\text{\{all variables\}}$
    \STATE $k=0$
    \WHILE{Repeat till $S$ contained $\text{maxVar}$ variables}
    \FORALL{$f_i \in F$}
    \STATE $R_i = J(\{S{(k)} + f_i\})$
    \ENDFOR
    \STATE $j=\text{arg} \max_{i} R_i$
    \IF{$R_j \geq J(S^{(k+1)})$}
    \STATE $k=k+1$
    \ELSE
    \STATE $S^{(k+1)} = \{S{(k)} + f_i\}$
    \STATE $k=k+1$
    \STATE $\text{flag}=1$
    \WHILE{$k > 2 \text{ and } \text{flag}=1$}
    \FORALL{$f_i \in S^{(k)}$}
    \STATE $R_i = J(\{S{(k)}\setminus f_i\})$
    \ENDFOR
    \STATE $j=\text{arg} \max_{i} R_i$
    \IF{$R_j < J(S^{(k-1)})$}
    \STATE $S^{(k-1)} = \{S{(k)} \setminus f_i\}$
    \STATE $k=k-1$
    \ELSE
    \STATE $\text{flag}=0$
    \ENDIF
    \ENDWHILE
    \ENDIF
    \ENDWHILE
    \RETURN $S$
    \end{algorithmic}
    \end{algorithm}

    \subsection{Implementation}
    In term of computational efficiency, the main issue is to find an efficient way to compute the mean vector and the covariance matrix and its inverse and to do it only when it is absolutely necessary. More specifically, we develop an optimized way to derive the submodel for cross-validation from the full model and we minimize the number of costly operations during a step of the selection algorithm.

        \subsubsection{Update for cross validation}

        Based on \cite{fauvel2015fast}, a method to accelerate the k-fold cross-validation process in the case of criterion functions based on correct classification measures was implemented. The idea is to estimate the GMM model with the whole training set and then, instead of training a model on $(k-1)$ folds, the complete model and the mean vector and the covariance matrix of the $k^{th}$ fold is used to derive the corresponding submodel.

        The following formulae can be obtained (details of calculation in Appendix \ref{app:cv_maj})
        \begin{align}
            \boldsymbol{\mu}_c^{n_c-\nu_c} &= \frac{n_c \boldsymbol{\mu}_c^{n_c} - \nu_c \boldsymbol{\mu}_c^{\nu_c}}{n_c - \nu_c} \\
            \boldsymbol{\Sigma}_c^{n_c-\nu_c} &= \frac{1}{n_c-\nu_c-1} ( (n_c-1) \boldsymbol{\Sigma}_c^{n_c} - (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} - \frac{n_c \nu_c}{(n_c-\nu_c)} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t)
        \end{align}
        where $\nu_c$ is the number of samples of class $c$ removed from the set.

        Additionally, the update of the GMM model is the occasion to precompute and store the constant $2 \log (\pi_c)$ which is a term of the decision function constant for a given training set.

        \subsubsection{Criterion function computation}

        As explained earlier, at each iteration the SFS and SFFS algorithms compute the value of a criterion function for every possible set of features composed of the selected ones augmented by one of the remaining features. One of the main achievements of this work is to reduce the computational time needed to compute the criterion function of augmented sets.

        We note $\boldsymbol{\Sigma}^{(k-1)}$ the covariance matrix of the $k-1^{th}$ iteration, i.e., the covariance of the selected features and $\boldsymbol{\Sigma}^{(k)}$ the covariance matrix of the $k^{th}$ iteration, i.e., the covariance of the augmented set. Then, the inverse of the covariance matrix $(\boldsymbol{\Sigma}^{(k)})^{-1}$, the quadratical term $(\mathbf{x}^{(k)})^t (\boldsymbol{\Sigma}^{(k)})^{-1} \mathbf{x}^{(k)}$ and the determinant $\log |\boldsymbol{\Sigma}^{(k)}|$ can be expressed in function of terms of the $(k-1)^{th}$ iteration. These calculi use the fact that the covariance matrix is a positive definite symmetric matrix to simplify formulae using matrices defined by blocks. These update rules are summed up hereafter and are available in \cite{webb2003statistical} (chapter 9.2).

        As $\boldsymbol{\Sigma}^{(k)}$ is a positive definite symmetric matrix, we can use the following notation
        \begin{equation*}
            \boldsymbol{\Sigma}^{(k)} =
            \bigg[\begin{array}{cc}
            \boldsymbol{\Sigma}^{(k-1)} & \mathbf{u}      \\
            \mathbf{u}^t          & \sigma_{kk} \\
            \end{array}\bigg]
        \end{equation*}
        where $\mathbf{u}$ is the $k^{th}$ column of the matrix without the diagonal element i.e. $\mathbf{u}_{i} = \boldsymbol{\Sigma}^{(k)}_{i+1,k}$ with $i \in [1,k-1]$.

        Using the formula of the inverse of a block matrix, the following formula expressing $(\boldsymbol{\Sigma}^{(k)})^{-1}$ in function of $(\boldsymbol{\Sigma}^{(k-1)})^{-1}$ is obtained
        \begin{equation}
            \fbox{$
            (\boldsymbol{\Sigma}^{(k)})^{-1} =
            \bigg[\begin{array}{cc}
            (\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} & - \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \\
            - \frac{1}{\alpha} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1}                                                                          & \frac{1}{\alpha}                  \\
            \end{array}\bigg]
            $}
        \end{equation}
        where $ \alpha = \sigma_{kk} - \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} $.

        In the case of a backward step in SFFS algorithm, we need to invert the formula in order to compute $(\boldsymbol{\Sigma}^{(k-1)})^{-1}$ knowing $(\boldsymbol{\Sigma}^{(k)})^{-1}$. Then, if we use the following notation for the previous matrix
        \begin{equation*}
            (\boldsymbol{\Sigma}^{(k)})^{-1} =
            \bigg[\begin{array}{cc}
            \mathbf{A}   & \mathbf{v} \\
            \mathbf{v}^t & \frac{1}{\alpha} \\
            \end{array}\bigg]
        \end{equation*}
        with $\mathbf{A} = (\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1}$ and $\mathbf{v} = - \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u}$, we can calculate
        \begin{align*}
            \mathbf{A} - \alpha \mathbf{v} \mathbf{v}^t
            &= (\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} - \alpha (- \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u}) (- \frac{1}{\alpha} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1})) \\
            &= (\boldsymbol{\Sigma}^{(k-1)})^{-1}
        \end{align*}

        \begin{equation}
            \fbox{$(\boldsymbol{\Sigma}^{(k-1)})^{-1} = \mathbf{A} - \alpha \mathbf{v} \mathbf{v}^t$}
        \end{equation}

        A formula can also be deduced for the quadratical term of the decision function. If we note $(\mathbf{x}^{(k)})^t = \left[\begin{array}{cc} (\mathbf{x}^{(k-1)})^t   & x^(k) \end{array}\right]$,
        \begin{align*}
            (\mathbf{x}^{(k)})^t (\boldsymbol{\Sigma}^{(k)})^{-1} \mathbf{x}^{(k)}
            &= \left[\begin{array}{cc} (\mathbf{x}^{(k-1)})^t   & x_k \end{array}\right]
            \left[\begin{array}{cc}
            \mathbf{A}   & \mathbf{v} \\
            \mathbf{v}^t & \frac{1}{\alpha}
            \end{array}\right]
            \left[\begin{array}{c} \mathbf{v} \\ x_k \end{array}\right] \\
            &= \left[\begin{array}{cc} (\mathbf{x}^{(k-1)})^t \mathbf{A} + x_k \mathbf{v}^t & (\mathbf{x}^{(k-1)})^t \mathbf{v} + \frac{x_k}{\alpha} \end{array}\right]
            \left[\begin{array}{c} \mathbf{v} \\ x_k \end{array}\right] \\
            &= (\mathbf{x}^{(k-1)})^t \mathbf{A} \mathbf{x}^{(k-1)} + x_k \mathbf{v}^t \mathbf{x}^{(k-1)} + (\mathbf{x}^{(k-1)})^t \mathbf{v} x_k + \frac{(x_k)^2}{\alpha} \\
            &= (\mathbf{x}^{(k-1)})^t ((\boldsymbol{\Sigma}^{(k-1)})^{-1} + \frac{1}{\alpha} (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{u} \mathbf{u}^t (\boldsymbol{\Sigma}^{(k-1)})^{-1}) \mathbf{x}^{(k-1)}
               + 2 x_k \mathbf{v}^t \mathbf{x}^{(k-1)} + \frac{(x_k)^2}{\alpha} \\
            &= (\mathbf{x}^{(k-1)})^t ((\boldsymbol{\Sigma}^{(k-1)})^{-1} + \alpha \mathbf{v} \mathbf{v}^t) \mathbf{x}^{(k-1)}
               + 2 x_k \mathbf{v}^t \mathbf{x}^{(k-1)} + \frac{(x_k)^2}{\alpha} \\
            &= (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( (\mathbf{x}^{(k-1)})^t \mathbf{v} \mathbf{v}^t \mathbf{x}^{(k-1)}
               + 2 \frac{x_k}{\alpha} \mathbf{v}^t \mathbf{x}^{(k-1)} + \frac{(x_k)^2}{\alpha^2}) \\
            &= (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( (\mathbf{x}^{(k-1)})^t \mathbf{v} + \frac{x_k}{\alpha} )^2 \\
            &= (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( \left[\begin{array}{cc} \mathbf{v}^t & \frac{1}{\alpha} \end{array}\right] \mathbf{x}^{(k)} )^2
        \end{align*}

        \begin{equation}
            \fbox{$(\mathbf{x}^{(k)})^t (\boldsymbol{\Sigma}^{(k)})^{-1} \mathbf{x}^{(k)} = (\mathbf{x}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} \mathbf{x}^{(k-1)} + \alpha ( \left[\begin{array}{cc} \mathbf{v}^t & \frac{1}{\alpha} \end{array}\right] \mathbf{x}^{(k)} )^2$}
        \end{equation}

        Finally, using the formula of the determinant of a block matrix and after taking the log, we obtain
        \begin{equation}
            \fbox{$\log \left(|\boldsymbol{\Sigma}^{(k)}|\right) = \log \left(|\boldsymbol{\Sigma}^{(k-1)}|\right) + \log \alpha$}
        \end{equation}

        To give an example, if a measure of good classification is used, we need to compute the decision rule and with the update rules, it can be rewritten as follow
        \begin{align*}
            Q(\mathbf{x}) = &- (\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)})^t (\boldsymbol{\Sigma}^{(k-1)})^{-1} (\mathbf{x}^{(k-1)} - \boldsymbol{\mu}^{(k-1)}) \\
            &- \alpha ( \left[\begin{array}{cc} \mathbf{v}^t & \frac{1}{\alpha} \end{array}\right] (\mathbf{x}^{(k)} - \boldsymbol{\mu}^{(k)}) )^2 \\
            &- \log \left(|\boldsymbol{\Sigma}^{(k-1)}|\right) - \log \alpha  + 2 \log (\pi)
        \end{align*}
        It can be noticed that this new formula allow us to precompute the computationally heavy terms because they are the same for all possible augmented sets at a given iteration of the selection. The Figure \ref{fig:opt_computation} illustrates the optimization of the selection algorithm.

        \begin{figure}[!ht]
            \centering
            \setlength{\tabcolsep}{2pt}
            \includegraphics[width=0.8\textwidth]{Fig/algo_iteration.png}
            \caption{Iteration of sequential forward features selection.\label{fig:opt_computation}}
        \end{figure}

        Nevertheless, during an iteration of the selection algorithm, it still needs to invert the covariance matrix of the previous iteration. In order to so, we choose to perform a decomposition in eigenvalues and eigenvectors using algorithm specific for symmetric matrices. In other words, we find $\boldsymbol{\Lambda}$ the diagonal matrix of eigenvalues and $\mathbf{Q}$ the matrix of eigenvectors s.t. $\boldsymbol{\Sigma} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^t$.

        This decomposition has several advantages. First, it is very simple to compute the determinant once the eigenvalues are available because the determinant is equal to the product of these values. Secondly, it is possible to check the eigenvalues and so to assure a better computational stability. Due to the curse of dimensionality, the covariance matrix is sometimes ill-estimated and the results is that some eigenvalues are really small (below computational precision) and the decomposition helps us to check their actual values and if they are below EPS\_FLT which the machine maximum precision for a float, we set these eigenvalues to EPS\_FLT.

% \section{Organization}
% \label{sec:orga}

%     \subsection{Objectives}

%     Several outcomes are expected of this internship. First, based on the previous work \cite{fauvel2015fast} of my tutor M.Fauvel, we want to {\bfseries reproduce the SFS selection method and develop an upgrade version} corresponding to the floating forward selection described in the previous section. This first part will results in a Python code freely available on Github \footnote{\url{https://github.com/Laadr/FFFS}}.

%     Then, the aim is collaborate with CNES in order to {\bfseries produce an external module which could be plugged in the open-source library Orfeo Toolbox (OTB) developed by CNES} (\cite{christophe2008orfeo}). This module implements the same methods as the Python version but has to be written in C++ and be compatible with the OTB. The module obtained is a fork of a template furnished by OTB developers and is also available freely on Github \footnote{\url{https://github.com/Laadr/otbExternalFastFeaturesSelection}}.

%     Finally, we want to {\bfseries test the efficiency of this selection method and compare it to a kernel-based method} (\cite{camps2010remote}). The testing is conducted first on hyperspectral images where features are directly the spectral band and secondly on remote sensing images where numerous and various features are used (texture features, HoG, morphological profiles, radiometric indexes, ...).

%     \subsection{Planning}

%     As can be expected, the cycle is conducted from high-level to low-level which means that the Python code is first developed and so allow us to set definitively the structure of the algorithm. The low-level C++ code is then developed in order to assure good performances and to respect the interface with the OTB.

%     Each implementation is followed by a testing period during which the code is tested with artificial and easily manipulable data. Moreover, the development of the C++ code is done in interaction with developers of the OTB and, in particular, a first meeting is organized at the early stage of the development to assure that the best choice are made and a second meeting is set during the validation of the C++ code to get feedbacks.

%     The experimentation are conducted at the end to demonstrate the interest of the project. The Gantt diagram \ref{gantt} summarized the expected organization of the project.

%     \begin{figure}[ht]
%         \centering
%         \resizebox{\linewidth}{!}{
%             \begin{ganttchart}[hgrid,
%                 vgrid,
%                 bar/.append style={fill=blue!30},
%                 group/.append style={draw=black,fill=red!50},
%                 title/.append style={fill=gray!20}]{1}{48}
%                 \gantttitle{\bfseries April}{8}\gantttitle{\bfseries May}{8}\gantttitle{\bfseries June}{8}\gantttitle{\bfseries July}{8}\gantttitle{\bfseries August}{8}\gantttitle{\bfseries September}{8} \\
%                 \ganttgroup{Development}{1}{26}\\
%                 \ganttbar{\em Python Implementation}{1}{8} \\
%                 \ganttbar{\em C++ Implementation}{9}{26} \\
%                 \ganttmilestone{\em Orfeo Toolbox Users Days (CNES)}{11}\\
%                 \ganttgroup{Testing}{7}{34}\\
%                 \ganttbar{\em Validation Python code}{7}{18}\\
%                 \ganttbar{\em Validation C++ code}{25}{34}\\
%                 \ganttmilestone{\em Meeting CNES}{31}\\
%                 \ganttgroup{Experimentation on real data}{20}{48}\\
%                 \ganttbar{\em Experimentation on hyperspectral images}{20}{48}\\
%                 \ganttbar{\em Experimentation with heterogeneous features}{32}{48}\\
%                 \ganttmilestone{\em Mid-term report}{24}\\
%                 \ganttmilestone{\em Final report}{47}
%             \end{ganttchart}}
%         \caption{Gantt diagram of the internship.}
%         \label{gantt}
%     \end{figure}



    \section{Experimental results about features selection with GMM}

    In this section, we discuss the results of the features selection method. The forward selection and the floating forward selection are compared and the results with the different criterion function are also discussed. All the experimentation has been conducted with a single dataset presented in the next subsection.

        \subsection{Aisa dataset}

        The Aisa dataset is composed of an hyperspectral image with 256 spectral band between 42 nm and 42 nm and a shapefile containing groundtruth. The groundtruth is made with 16 classes representing landcover of the area. A representation of the dataset is displayed in Figure \ref{fig:aisa}.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.5\textwidth]{Fig/aisa.png} &
                \includegraphics[width=0.5\textwidth]{Fig/aisa_gt.png} \\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \end{tabular}
            \caption{Aisa dataset : {\bfseries (a)} color composition and {\bfseries (b)} groundtruth.\label{fig:aisa}}
        \end{figure}

        \subsection{Experimental results}

        For the results discussed in this section, the selection process was repeated 20 times with each time a different training set and the classification rate and time processing presented hereafter are the means over these 20 trials.

        % discuss evolution of classif rate with nb of selected var
        Figure \ref{fig:res-1} corresponds to the evolution of the classification rate in function of the number of selected variables with the three metrics introduced before (overall accuracy, Cohen's kappa and mean of F1-score). We observe that at first the classification rate rapidly increase before stabilizing and then slowly decrease. This is the typical behavior expected when using features selection and a GMM classifier and we want to use the number of variables which maximizes the classification rate even if the maximum is not always easily found for example because of local maxima.


        \begin{figure}[!ht]
            \centering
            \begin{tabular}{c}
                \includegraphics[width=0.5\textwidth]{Fig/aisa.png}\\
            \end{tabular}
            \caption{Classification results (overall accuracy, Cohen's kappa and mean of F1-score) averaged on 20 trials using forward selection and Jeffries-Matusita distance as criterion.\label{fig:res-1}}
        \end{figure}


        % discuss evolution with training set size
        It is also interesting to see the influence of the size of the training set. From Figure \ref{fig:setsize}, we see that, either with an equalized training set or with a proportional training set, the classification rate is better with more samples and also that the maximum is reached with more selected variables which that more samples allows to evaluate more precisely the relevant variables.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{c}
                \includegraphics[width=0.5\textwidth]{Fig/aisa.png}\\
            \end{tabular}
            \caption{Classification results averaged on 20 trials using forward selection and Jeffries-Matusita distance as criterion: {\bfseries (a)} with 50, 100, 200 samples by class in training set and {\bfseries (b)} with 0.5\%, 1\%, 2.5\% of dataset in training set keeping proportion.\label{fig:res-1}}
        \end{figure}

        % discuss sfs vs sffs
        Using Figure \ref{fig:sfs-vs-sffs}, we can compare the results when using forward selection or SFFS. And we see that with this particular dataset, the SFFS does not produce better results than the other method. One should be careful not to draw conclusion too fast. It does not mean that the SFFS method has no advantages but rather that the SFFS advantages are highly related to the dataset structure and thus it is always good to check if the basic forward method is sufficient for a given dataset.

        \begin{figure}[!ht]
            \centering
            \begin{tabular}{cc}
                \includegraphics[width=0.5\textwidth]{Fig/aisa.png} &
                \includegraphics[width=0.5\textwidth]{Fig/aisa_gt.png} \\
                {\bfseries{(a)}} & {\bfseries{(b)}} \\
            \end{tabular}
            \caption{Classification results with 20 trials : {\bfseries (a)} using Jeffries-Matusita distance as criterion and {\bfseries (b)} using Cohen's kappa as criterion.\label{fig:sfs-vs-sffs}}
        \end{figure}

        % discuss criterion advantages
        Now, 


        % discuss time processing








    % \section{Implementation of the Orfeo Toolbox module}

    % After the calibration of the algorithm with a Python implementation, we aim to realise a C++ implementation of this selection method. And, to make it easily available, we choose to develop a remote module for the Orfeo Toolbox (OTB) designed by the CNES. The current section describes all the choice made during the C++ implementation.

    %     \subsection{Integration in Orfeo Toolbox}

    %     The module developed is available on Github \footnote{\url{https://github.com/Laadr/otbExternalFastFeaturesSelection}} and is actually a fork of the template for remote module furnished by OTB developers.

    %     We choose to implement our own version of a GMM classifier in a class named \emph{GMMMachineLearningModel} in order to assure better performance in term of computational time. The second step is to implement a subclass of the GMM classifier including the features selection method.

    %     \subsection{GMM classifier}

    %     The \emph{GMMMachineLearningModel} class inherits from the OTB class \emph{MachineLearningModel}. It implements the virtual method \emph{Train()} and \emph{Predict()} inherited from the super class and on additional method \emph{Decomposition()} used to decompose a symmetric matrix in eigenvalues and eigenvectors.

    %     In this class, all measurement vectors are of type \emph{itk::VariableLengthVector} and all other vectors are of type \emph{std::vector}. The various matrix are of type \emph{itk::VariableSizeMatrix}.

    %     The super class of GMMMachineLearningModel enables the management of a list of samples used for training. This class includes 6 members describing the model and computed in the \emph{Train()} function : the number of class, the number of features, a vector containing the proportion of each class in the training set, a vector containing the number of samples in each class, a vector containing the mean vector of each class and finally a vector containing the covariance matrix of each class.

    %     Moreover, the decision function used for prediction is rewritted in order to assure faster computation. We use the following equivalent of Equation \ref{eq:decision}
    %     \begin{equation}
    %         Qc(\mathbf{x}) = -  || \boldsymbol{\Lambda}_c^{-\frac{1}{2}} \boldsymbol{Q}_c^t (\mathbf{x} - \boldsymbol{\mu}_c) ||^2 - \log (|\boldsymbol{\Sigma}_c|) + 2 \log (\pi_c)
    %     \end{equation}
    %     where $\boldsymbol{\Lambda}_c$ and $\boldsymbol{Q}_c$ are obtained in decomposing $\boldsymbol{\Sigma}_c$ in eigenvalues and eigenvectors s.t. $\boldsymbol{\Sigma}_c = \boldsymbol{Q}_c \boldsymbol{\Lambda}_c \boldsymbol{Q}_c^t$. Thus, we can precompute several terms of the decision function in the \emph{Train()} function. Eigenvalues, eigenvectors, the terms $\boldsymbol{\Lambda}_c^{-\frac{1}{2}} \boldsymbol{Q}_c^t$ and the terms $- \log (|\boldsymbol{\Sigma}_c|) + 2 \log (\pi_c)$ are stored in vectors as class members.

    %     The decompositions in eigenvalues and eigenvectors of the covariance matrices are performed with the \emph{Decomposition()} function which is a wrapper to instance the class \emph{itk::symmetricEigenAnalysis} and call its method \emph{ComputeEigenValuesAndVectors} (this method uses netlib routines). Immediately after decomposition, eigenvalues are examined and all eigenvalues inferior to EPSILON\_FLT (or EPSILON\_DBL) are reset to this value. The idea is to limit computational error when parameters are ill-estimated due to an insufficient amount of training samples.

    %     The Ridge regularization described in Section \ref{sec:regularization} is also implemented in this class. If the value of $\tau$ is set before training, the regularization is performed during the precomputation of the terms of the decision function and if $\tau$ is modified after training, the function \emph{SetTau()} updates the precomputation in addition to changing the value of $\tau$.










\newpage

\appendix
\section{Update for cross validation (calculation)}
\label{app:cv_maj}

    \subsection{Mean update}
        \begin{align*}
            \boldsymbol{\mu}_c^{n_c} &= \frac{1}{n_c} \sum_{j = 1}^{n_c} \mathbf{x}_j \\
                        &= \frac{1}{n_c} \sum_{j = 1}^{n_c-\nu_c} \mathbf{x}_j + \frac{1}{n_c} \sum_{j = n_c-\nu_c +1}^{n_c} \mathbf{x}_j \\
                        &= \frac{n_c-\nu_c}{n_c} \boldsymbol{\mu}_c^{n_c-\nu_c} + \frac{\nu_c}{n_c} \boldsymbol{\mu}_c^{\nu_c} \\
                        &= \boldsymbol{\mu}_c^{n_c-\nu_c} + \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})
        \end{align*}

        \begin{equation}
             \fbox{$\boldsymbol{\mu}_c^{n_c} = \boldsymbol{\mu}_c^{n_c-\nu_c} + \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})$}
        \end{equation}

        \begin{equation}
            \fbox{$\boldsymbol{\mu}_c^{n_c-\nu_c} = \frac{n_c \boldsymbol{\mu}_c^{n_c} - \nu_c \boldsymbol{\mu}_c^{\nu_c}}{n_c - \nu_c}$}
        \end{equation}

    \subsection{Covariance matrix update}
        \begin{align*}
            \boldsymbol{\Sigma}_c^{n_c} &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c})^t \\
                           &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c} - \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c} - \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c}))^t \\
                           &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                                                                 &\qquad + \frac{\nu_c^2}{n_c^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                                                                 &\qquad - \frac{\nu_c}{n_c} (\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                                                                 &\qquad - \frac{\nu_c}{n_c} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})^t
        \end{align*}

        \begin{itemize}
            \item $(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t = \frac{n_c^2}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$
            \item $\frac{1}{n_c} \sum_{j = 1}^{n_c} (\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t = \frac{n_c \nu_c}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$
            \item $\frac{1}{n_c} \sum_{j = 1}^{n_c} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\mathbf{x}_j-\boldsymbol{\mu}_c^{n_c-\nu_c})^t = \frac{n_c \nu_c}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$
        \end{itemize}

        \begin{align*}
            \boldsymbol{\Sigma}_c^{n_c} &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t - \frac{\nu_c^2}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
                           &= \frac{1}{n_c - 1} \sum_{j = 1}^{n_c-\nu_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t + \frac{1}{n_c - 1} \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                           &\qquad - \frac{n_c \nu_c^2}{(n_c-1)(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
                           &= \frac{n_c-\nu_c-1}{n_c-1} \boldsymbol{\Sigma}_c^{n_c-\nu_c} + \frac{1}{n_c - 1} \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t \\
                           &\qquad - \frac{n_c \nu_c^2}{(n_c-1)(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
        \end{align*}

        \begin{align*}
            \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{n_c-\nu_c})^t
                &= \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c} + \boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c} + \boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})^t\\
                &= (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} + \nu_c (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c-\nu_c})^t\\
                &\qquad + \sum_{j = n_c-\nu_c+1}^{n_c} (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c}) (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c})^t\\
                &\qquad + \sum_{j = n_c-\nu_c+1}^{n_c} (\boldsymbol{\mu}_c^{\nu_c} - \boldsymbol{\mu}_c^{n_c-\nu_c}) (\mathbf{x}_j - \boldsymbol{\mu}_c^{\nu_c})^t\\
                &= (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} + \frac{n_c^2 \nu_c}{(n_c-\nu_c)^2} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t \\
        \end{align*}

        \begin{equation}
            \fbox{$\boldsymbol{\Sigma}_c^{n_c} = \frac{n_c-\nu_c-1}{n_c-1} \boldsymbol{\Sigma}_c^{n_c-\nu_c} + \frac{\nu_c-1}{n_c-1} \boldsymbol{\Sigma}_c^{\nu_c} + \frac{n_c \nu_c}{(n_c-1)(n_c-\nu_c)} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$}
        \end{equation}

        \begin{equation}
            \fbox{$\boldsymbol{\Sigma}_c^{n_c-\nu_c} = \frac{1}{n_c-\nu_c-1} ( (n_c-1) \boldsymbol{\Sigma}_c^{n_c} - (\nu_c-1) \boldsymbol{\Sigma}_c^{\nu_c} - \frac{n_c \nu_c}{(n_c-\nu_c)} (\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})(\boldsymbol{\mu}_c^{\nu_c}-\boldsymbol{\mu}_c^{n_c})^t$ )}
        \end{equation}


\bibliographystyle{ieeetr}
\bibliography{biblio}

\end{document}
